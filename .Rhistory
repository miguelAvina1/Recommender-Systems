barplot(height = table(places$placeID), xlab = "placeID", ylab = "Ratings received")
library('plyr')
t <- count(places, 'placeID')
sprintf("Average votes per placeID: %s", mean(t$freq))
summary(t$freq)
boxplot(t$freq)
barplot(height = table(places$userID), xlab = "userID", ylab = "# of places rated")
t <- count(places, 'userID')
sprintf("Average places rated per User: %s", mean(t$freq))
summary(t$freq)
boxplot(t$freq)
hist(places$rating, xlab = 'Rating', main = 'Histogram of "Rating"')
plot(density(places$rating), main = "Density plot for 'rating'")
places$rating <- as.factor(places$rating)
summary(factor(places$rating))
prop.table(table(places$rating))
head(places$name)
sprintf("Number of cities: %s",nlevels(places$city))
levels(places$city)
places$city <- factor(places$city, labels=c("Undefined","Cd. Victoria","Cd. Victoria","Cd. Victoria",
"Cuernavaca","Cuernavaca","Jiutepec", "San Luis Potosi",
"San Luis Potosi", "San Luis Potosi", "San Luis Potosi",
"San Luis Potosi", "San Luis Potosi", "San Luis Potosi",
"Soledad", "Cd. Victoria", "Cd. Victoria"))
levels(places$city)
prop.table(table(places$city))
barplot(height = table(places$city), las=2)
t <- count(places, 'city')
summary(t$freq)
boxplot(t$freq)
print(summary(places$latitude))
sprintf("Std dev: %s", round(sd(places$latitude, na.rm = TRUE), digits = 4))
hist(places$latitude)
plot(density(places$latitude, na.rm = TRUE))
print(summary(places$longitude))
sprintf("Std dev: %s", round(sd(places$longitude, na.rm = TRUE), digits = 4))
hist(places$longitude)
plot(density(places$longitude, na.rm = TRUE))
plot(places$longitude, places$latitude)
barplot(height = table(places$cuisine), las=2)
t <- count(places, 'cuisine')
sprintf("Average places per cuisine: %s", mean(t$freq))
summary(t$freq)
outvals <- boxplot(t$freq)$out
t[which(t$freq %in% outvals),]
#install.packages("revgeo")
#install.packages("ggmap")
library(revgeo)
library(ggmap)
undef_cities <- filter(places, city == 'Undefined')
undef_cities <- select(.data = undef_cities, placeID, city, latitude, longitude)
places$city <- as.character(places$city)
for (i in 1:length(places$city)) {
if (places$city[i] == 'Undefined') {
possibleCity <- revgeo(places$longitude[i], places$latitude[i])
places$city[i] =  unlist(strsplit(as.character(possibleCity), ", "))[2]
}
}
places$city <- as.factor(places$city)
sprintf("Number of places: %s",nlevels(places$city))  #
levels(places$city)
places$city <- factor(places$city, labels=c("Cd. Victoria","Cd. Victoria","Cuernavaca","Jiutepec",
"San Luis Potosi", "San Luis Potosi", "Soledad", "Soledad"))
places$city <- droplevels(places$city)
sprintf("Number of places: %s",nlevels(places$city))  #
levels(places$city)
register_google(key = "AIzaSyD4mKmiXYdtE9xALBocEVu_Tl15a4iEW4I")
places$GG_result <-  'Not compared'
for (i in 1:length(places$city)) {
#
# SO, FOR GG MAP, GGMAP returns a list with all posible addresses it found. However the structure is NOT the same for each
# address it returns. So, we can not search of a given element inside the lists of lists it returns. We would need to use another
# approach. One non-efficient but useful approach would be to loop inside all the info we get when calling revgeocode to search
# for any coincidence to the data we retrieved from RevGeo, if we find it, then we can be sure we get the correct city
# using revgeo. Otherwise, we would need to do a manual inspection of the data received from revgeocode
# The search would be basically a deep-first search
if (places$placeID[i] %in% undef_cities$placeID) {   # We are just going to comprobate with those cities that were undefined before
google_result <- revgeocode(c(places$longitude[i], places$latitude[i]), output = "all")$results
found <- FALSE
citynameOriginal <- places$city[i]
compare_city <- as.character(lapply(as.character(citynameOriginal), tolower))
compare_city <- gsub("[^a-z]","",compare_city)
# print(compare_city)
#sprintf("Comparing with '%s'", compare_city)
for (j in 1:length(google_result)) {
address_comp <- google_result[[j]]$address_components
for (k in 1:length(address_comp)) {
long_name <- as.character(lapply(as.character(address_comp[[k]]$long_name), tolower))
long_name <- gsub("[^a-z]","",long_name)
short_name <- as.character(lapply(as.character(address_comp[[k]]$short_name), tolower))
short_name <- gsub("[^a-z]","",short_name)
# We are not going to compae if the compare_city is at least substring of the google's result
# We are not comparing it letter to letter because we found that google returns 'Soledad de Graciano Sanchez'
# instead of just 'Soledad', which is the name of the city we got from revgeo. Therefore, comparing it using the
# '==' operator will return FALSE, and it should be true, So we are going to use grep to check if one is substring
# of the other
if (grepl(compare_city, long_name, fixed = TRUE) || grepl(compare_city, short_name, fixed = TRUE)) {
# print("FOUND:")
# print(long_name)
found = TRUE
break
}
}
if (found == TRUE) {
break
}
}
if (found == TRUE) {
places$GG_result[i] = as.character(citynameOriginal)  # Just put the same city
} else {
places$GG_result[i] <- 'Not the same'  # FUrther analysis will be needed
}
}
}
differentResults <- filter(.data = places, GG_result == 'Not the same')
differentResults <- select(differentResults, -userID, -rating)
differentResults <- unique(differentResults)
differentResults
sprintf("There are: %s cities",nlevels(places$city))
levels(places$city)
t <- count(places, 'placeID')  # Getting frequency of each placeID
t <- merge(t, places, by='placeID')
t <- select(t, placeID, freq, name)
t <- unique(x = t)
t <- t[order(-t$freq),]  # Ordering in descending order
popularPlaceID <- t$placeID[1:10]  # Here we have the 10 most popular places ID
mostPopular <- subset(places, placeID %in% popularPlaceID) # Here we extract the entire row where the
# most popular placeID appears. Now we just want 1 row per name
mostPopular <- droplevels(unique(mostPopular$name))
mostPopular <- data.frame(popularPlaceID, mostPopular, t$freq[1:10])
colnames(mostPopular) <- c("placeID", "Restaurant", "Votes")
mostPopular
best_rated <- filter(places, rating == "2")
best_rated <- select(.data = best_rated, placeID, name)
t <- count(best_rated, 'placeID')  # Getting frequency of each best-rated placeID
t <- t[order(-t$freq),]
bestRatedPlaceID <- t$placeID[1:10]  # Here we have the 10 most popular and best rated places ID
bestRated <- subset(best_rated, placeID %in% bestRatedPlaceID) # Here we extract the entire row of top 10 best rated places
bestRated <- droplevels(unique(bestRated$name))
bestRated <- data.frame(popularPlaceID, bestRated, t$freq[1:10])
colnames(bestRated) <- c("placeID", "Restaurant", "Positive Votes")
bestRated
nRestaurants <- nlevels(places$placeID)
sprintf("In this data set we have a total of %s different restaurants", nRestaurants)
mostPopular <- merge(mostPopular, cuisine, by='placeID')
mostPopular
levels(droplevels(mostPopular$cuisine))
sprintf("Number of types of cuisines: %s",nlevels(droplevels(places$cuisine)))
levels(places$cuisine)
cuisine_original   <- read.csv(file = "RCdata/chefmozcuisine.csv", header = TRUE, sep = ',')
cuisine_original   <- select(.data = cuisine_original, placeID, Rcuisine)
sprintf("Number of types of cuisine: %s",nlevels(cuisine_original$Rcuisine))
commonPlaceIDs <- as.factor(intersect(cuisine$placeID, rating$placeID))
cuisine_original_130 <- droplevels(subset(cuisine_original, placeID %in% commonPlaceIDs))
sprintf("Number of types of cuisine: %s", nlevels(cuisine_original_130$Rcuisine))
levels(cuisine_original_130$Rcuisine)
places_stm <- select(places, placeID, userID, rating, name)
head(places_stm)
places_stm$rating <- as.numeric(places_stm$rating)
utMx <- with(places_stm, tapply(rating, list(userID, name), sum, default=NA))
sprintf("Dimension of the matrix: %s",dim(utMx))
n_occur <- data.frame(table((geoPlaces$name)))
n_occur[n_occur$Freq > 1,]
geoPlaces[geoPlaces$name %in% n_occur$Var1[n_occur$Freq > 1],][,1:2]
# Let's get the placeID of the first one
ID <- geoPlaces[geoPlaces$name %in% n_occur$Var1[n_occur$Freq > 1],][1,1]
ID
backup <- places$name
places$name <- as.character(places$name)
for (i in 1:length(places$placeID)) {
if (places$placeID[i] == ID) {
places$name[i] = "Gorditas Dona Tota 2"
}
}
places$name <- as.factor(places$name)
places_stm <- select(places, placeID, userID, rating, name)
places_stm$rating <- as.numeric(places_stm$rating)
utMx <- with(places_stm, tapply(rating, list(userID, name), sum, default=NA))
dim(utMx)
utMx[1:10, 3:6]  # Here we can observe that it is a disperse matrix
place_names <- colnames(utMx)
head(place_names)
write.csv(utMx, file = "RCdata/Utility_Matrix.csv", na="")
#install.packages("irlba")
library(irlba)
utMx[is.na(utMx)] <- 0
MSVD <- svd(t(utMx))  # in MSVD we have u, d and v that forms from the decomposition
dim(MSVD$u)  # We can observe we end with the right size of u, because we transpose the utility matrix in the line before
placesLatentMatix <- MSVD$u
placesLatentMatix[1:5,1:7]
utrunc <- MSVD$u[,1:10]
corr_mx <- cor( t(utrunc), use='pairwise.complete.obs', method = "pearson")
df_cor <- as.data.frame(corr_mx)
library(ggplot2)
library(reshape2)
tmp <- melt(data.matrix(cor(df_cor)))
ggplot(data=tmp, aes(x=Var1,y=Var2,fill=value)) + geom_tile()
place_names <- colnames(utMx)
# We select the name
name2look <- 'Gorditas Doa Gloria'
#And obtain the index of the correlation matrix
nameIndex <- which(place_names == name2look)
restOptions <- corr_mx[,nameIndex]
# We obtain the index organized from higher correlation to lower correlation
orderedRest <- order(-restOptions)
# And then we use these indices to extract the names associated with those indices with greater correlation
# We ignore the first index since it will be the one with correlation 1, that is, the same restaurant and we will not recommend the same restaurant
numberOfRecomm <- 12
RecommendedRest <- place_names[orderedRest[2:numberOfRecomm+1]]
RecommendedRest # Here are the 12 recommended restaurants
plot(MSVD$d)
RecommendedRest[1]
RecommendedRest[0]
for (i in 1:12){
print(RecommendedRest[i])
}   # Here are the 12 recommended restaurants
for (i in 1:11){
print(RecommendedRest[i])
}   # Here are the 12 recommended restaurants
restOptions <- corr_mx[,nameIndex]
# We obtain the index organized from higher correlation to lower correlation
orderedRest <- order(-restOptions)
# And then we use these indices to extract the names associated with those indices with greater correlation
# We ignore the first index since it will be the one with correlation 1, that is, the same restaurant and we will not recommend the same restaurant
numberOfRecomm <- 12
RecommendedRest <- place_names[orderedRest[2:numberOfRecomm+1]]
for (i in 1:11){
print(RecommendedRest[i])
}   # Here are the 12 recommended restaurants
geoPlaces <- read.csv(file = "RCdata/geoplaces2.csv", header = TRUE, sep = ',')
rating    <- read.csv(file = "RCdata/rating_final.csv", header = TRUE, sep = ',')
cuisine   <- read.csv(file = "RCdata/chefmozcuisine.csv", header = TRUE, sep = ',')
library(dplyr)
geoPlaces <- select(.data = geoPlaces, placeID, name, city, latitude, longitude)
rating    <- select(.data = rating, userID, placeID, rating)
cuisine   <- select(.data = cuisine, placeID, Rcuisine)
head(geoPlaces)
head(rating)
head(cuisine)
sprintf("Number of Observations in cuisine: %s",dim(cuisine)[1])
sprintf("Number of Observations in geoPlaces: %s",dim(geoPlaces)[1])
sprintf("Number of Observations in rating: %s",dim(rating)[1])
rating_UserIDs <- unique(rating$userID)
sprintf("Number of unique userID: %s", nlevels(rating_UserIDs))
rating$placeID <- as.factor(rating$placeID)
sprintf("Number of unique placeID: %s",nlevels(rating$placeID))
n_occur <- data.frame(table((geoPlaces$name)))
colnames(n_occur) <- c("Restaurant_name", "Freq")
n_occur[n_occur$Freq > 1,]
geoPlaces[geoPlaces$name %in% n_occur$Restaurant_name[n_occur$Freq > 1],]
cuisine$placeID <- as.factor(cuisine$placeID)
sprintf("Number of different placeID: %s",nlevels(cuisine$placeID))
commonPlaceIDs <- as.factor(intersect(cuisine$placeID, rating$placeID))
sprintf("Intersection between rating and cuisine: %s",nlevels(commonPlaceIDs))
n_occur <- data.frame(table(cuisine$placeID))
n_occur <- n_occur[order(-n_occur$Freq),]
colnames(n_occur) <- c("placeID", "Freq")
head(n_occur[n_occur$Freq > 1,])
cuisine[cuisine$placeID %in% n_occur$placeID[n_occur$Freq > 1],][1:10,]
filter(cuisine, placeID == n_occur$placeID[1])
places <- merge(rating, geoPlaces, by="placeID")
sprintf("Number of observations in merged dataframe: %s",dim(places)[1])
z <-  cuisine[cuisine$placeID %in% n_occur$placeID[n_occur$Freq > 1],]
examplePlaceID <- intersect(commonPlaceIDs, z$placeID)[1]
filter(rating, placeID == examplePlaceID)
examplePair <- filter(rating, placeID == examplePlaceID)[1,1:3]
exampleUserID <- (examplePair[1])
examplePair
badMerge <- merge(places, cuisine, by="placeID")
z <- filter(badMerge, placeID == examplePlaceID)
z
filter(z, userID ==  exampleUserID[1,1])
cuisine <- filter(cuisine, placeID %in% commonPlaceIDs)
merged_Places <- list()
merged_Cuisines <- list()
#cuisine_merged_Places <- c(cuisine_merged_Places, 3)
for (place in commonPlaceIDs) { # We will iterate along each placeID we have
currentPlaceID <- filter(cuisine, placeID == place)    # and will examine one by one
new_level2 <- as.character(currentPlaceID$Rcuisine[1])
currentPlaceID <- currentPlaceID[-c(1),]
for (cuisineKind in currentPlaceID$Rcuisine) {
new_level2 <- paste(new_level2, as.character(cuisineKind), sep='_')
}
merged_Places <- c(merged_Places, place)
merged_Cuisines <- c(merged_Cuisines, new_level2)
}
cuisine <- data.frame('placeID' = unlist(merged_Places, use.names = FALSE), 'cuisine'=unlist(merged_Cuisines, use.names = FALSE) )
rm(merged_Places)
rm(merged_Cuisines)
rm(currentPlaceID)
places_drop <- merge(places, cuisine, by="placeID")
uniquePlaceID <- droplevels(as.factor(unique(places_drop$placeID)))
places <- merge(places, cuisine, by="placeID", all=TRUE)
levels(places[,8]) <- c(levels(places[,8]),"?")   # we are adding '?' to tell that we dont have that info
places[is.na(places)] <- '?'
head(places)
summary(places)
sprintf("Number of places in the dataset: %s",nlevels(places$placeID))
head(prop.table(table(places$placeID)))
barplot(height = table(places$placeID), xlab = "placeID", ylab = "Ratings received")
library('plyr')
t <- count(places, 'placeID')
sprintf("Average votes per placeID: %s", mean(t$freq))
summary(t$freq)
boxplot(t$freq)
barplot(height = table(places$userID), xlab = "userID", ylab = "# of places rated")
t <- count(places, 'userID')
sprintf("Average places rated per User: %s", mean(t$freq))
summary(t$freq)
boxplot(t$freq)
hist(places$rating, xlab = 'Rating', main = 'Histogram of "Rating"')
plot(density(places$rating), main = "Density plot for 'rating'")
places$rating <- as.factor(places$rating)
summary(factor(places$rating))
prop.table(table(places$rating))
head(places$name)
sprintf("Number of cities: %s",nlevels(places$city))
levels(places$city)
places$city <- factor(places$city, labels=c("Undefined","Cd. Victoria","Cd. Victoria","Cd. Victoria",
"Cuernavaca","Cuernavaca","Jiutepec", "San Luis Potosi",
"San Luis Potosi", "San Luis Potosi", "San Luis Potosi",
"San Luis Potosi", "San Luis Potosi", "San Luis Potosi",
"Soledad", "Cd. Victoria", "Cd. Victoria"))
levels(places$city)
prop.table(table(places$city))
barplot(height = table(places$city), las=2)
t <- count(places, 'city')
summary(t$freq)
boxplot(t$freq)
print(summary(places$latitude))
sprintf("Std dev: %s", round(sd(places$latitude, na.rm = TRUE), digits = 4))
hist(places$latitude)
plot(density(places$latitude, na.rm = TRUE))
print(summary(places$longitude))
sprintf("Std dev: %s", round(sd(places$longitude, na.rm = TRUE), digits = 4))
hist(places$longitude)
plot(density(places$longitude, na.rm = TRUE))
plot(places$longitude, places$latitude)
barplot(height = table(places$cuisine), las=2)
t <- count(places, 'cuisine')
sprintf("Average places per cuisine: %s", mean(t$freq))
summary(t$freq)
outvals <- boxplot(t$freq)$out
t[which(t$freq %in% outvals),]
#install.packages("revgeo")
#install.packages("ggmap")
library(revgeo)
library(ggmap)
undef_cities <- filter(places, city == 'Undefined')
undef_cities <- select(.data = undef_cities, placeID, city, latitude, longitude)
places$city <- as.character(places$city)
for (i in 1:length(places$city)) {
if (places$city[i] == 'Undefined') {
possibleCity <- revgeo(places$longitude[i], places$latitude[i])
places$city[i] =  unlist(strsplit(as.character(possibleCity), ", "))[2]
}
}
places$city <- as.factor(places$city)
sprintf("Number of places: %s",nlevels(places$city))  #
levels(places$city)
places$city <- factor(places$city, labels=c("Cd. Victoria","Cd. Victoria","Cuernavaca","Jiutepec",
"San Luis Potosi", "San Luis Potosi", "Soledad", "Soledad"))
places$city <- droplevels(places$city)
sprintf("Number of places: %s",nlevels(places$city))  #
levels(places$city)
register_google(key = "AIzaSyD4mKmiXYdtE9xALBocEVu_Tl15a4iEW4I")
places$GG_result <-  'Not compared'
for (i in 1:length(places$city)) {
#
# SO, FOR GG MAP, GGMAP returns a list with all posible addresses it found. However the structure is NOT the same for each
# address it returns. So, we can not search of a given element inside the lists of lists it returns. We would need to use another
# approach. One non-efficient but useful approach would be to loop inside all the info we get when calling revgeocode to search
# for any coincidence to the data we retrieved from RevGeo, if we find it, then we can be sure we get the correct city
# using revgeo. Otherwise, we would need to do a manual inspection of the data received from revgeocode
# The search would be basically a deep-first search
if (places$placeID[i] %in% undef_cities$placeID) {   # We are just going to comprobate with those cities that were undefined before
google_result <- revgeocode(c(places$longitude[i], places$latitude[i]), output = "all")$results
found <- FALSE
citynameOriginal <- places$city[i]
compare_city <- as.character(lapply(as.character(citynameOriginal), tolower))
compare_city <- gsub("[^a-z]","",compare_city)
# print(compare_city)
#sprintf("Comparing with '%s'", compare_city)
for (j in 1:length(google_result)) {
address_comp <- google_result[[j]]$address_components
for (k in 1:length(address_comp)) {
long_name <- as.character(lapply(as.character(address_comp[[k]]$long_name), tolower))
long_name <- gsub("[^a-z]","",long_name)
short_name <- as.character(lapply(as.character(address_comp[[k]]$short_name), tolower))
short_name <- gsub("[^a-z]","",short_name)
# We are not going to compae if the compare_city is at least substring of the google's result
# We are not comparing it letter to letter because we found that google returns 'Soledad de Graciano Sanchez'
# instead of just 'Soledad', which is the name of the city we got from revgeo. Therefore, comparing it using the
# '==' operator will return FALSE, and it should be true, So we are going to use grep to check if one is substring
# of the other
if (grepl(compare_city, long_name, fixed = TRUE) || grepl(compare_city, short_name, fixed = TRUE)) {
# print("FOUND:")
# print(long_name)
found = TRUE
break
}
}
if (found == TRUE) {
break
}
}
if (found == TRUE) {
places$GG_result[i] = as.character(citynameOriginal)  # Just put the same city
} else {
places$GG_result[i] <- 'Not the same'  # FUrther analysis will be needed
}
}
}
differentResults <- filter(.data = places, GG_result == 'Not the same')
differentResults <- select(differentResults, -userID, -rating)
differentResults <- unique(differentResults)
differentResults
sprintf("There are: %s cities",nlevels(places$city))
levels(places$city)
t <- count(places, 'placeID')  # Getting frequency of each placeID
t <- merge(t, places, by='placeID')
t <- select(t, placeID, freq, name)
t <- unique(x = t)
t <- t[order(-t$freq),]  # Ordering in descending order
popularPlaceID <- t$placeID[1:10]  # Here we have the 10 most popular places ID
mostPopular <- subset(places, placeID %in% popularPlaceID) # Here we extract the entire row where the
# most popular placeID appears. Now we just want 1 row per name
mostPopular <- droplevels(unique(mostPopular$name))
mostPopular <- data.frame(popularPlaceID, mostPopular, t$freq[1:10])
colnames(mostPopular) <- c("placeID", "Restaurant", "Votes")
mostPopular
best_rated <- filter(places, rating == "2")
best_rated <- select(.data = best_rated, placeID, name)
t <- count(best_rated, 'placeID')  # Getting frequency of each best-rated placeID
t <- t[order(-t$freq),]
bestRatedPlaceID <- t$placeID[1:10]  # Here we have the 10 most popular and best rated places ID
bestRated <- subset(best_rated, placeID %in% bestRatedPlaceID) # Here we extract the entire row of top 10 best rated places
bestRated <- droplevels(unique(bestRated$name))
bestRated <- data.frame(popularPlaceID, bestRated, t$freq[1:10])
colnames(bestRated) <- c("placeID", "Restaurant", "Positive Votes")
bestRated
nRestaurants <- nlevels(places$placeID)
sprintf("In this data set we have a total of %s different restaurants", nRestaurants)
mostPopular <- merge(mostPopular, cuisine, by='placeID')
mostPopular
levels(droplevels(mostPopular$cuisine))
sprintf("Number of types of cuisines: %s",nlevels(droplevels(places$cuisine)))
levels(places$cuisine)
cuisine_original   <- read.csv(file = "RCdata/chefmozcuisine.csv", header = TRUE, sep = ',')
cuisine_original   <- select(.data = cuisine_original, placeID, Rcuisine)
sprintf("Number of types of cuisine: %s",nlevels(cuisine_original$Rcuisine))
commonPlaceIDs <- as.factor(intersect(cuisine$placeID, rating$placeID))
cuisine_original_130 <- droplevels(subset(cuisine_original, placeID %in% commonPlaceIDs))
sprintf("Number of types of cuisine: %s", nlevels(cuisine_original_130$Rcuisine))
levels(cuisine_original_130$Rcuisine)
places_stm <- select(places, placeID, userID, rating, name)
head(places_stm)
places_stm$rating <- as.numeric(places_stm$rating)
utMx <- with(places_stm, tapply(rating, list(userID, name), sum, default=NA))
sprintf("Dimension of the matrix: %s",dim(utMx))
n_occur <- data.frame(table((geoPlaces$name)))
n_occur[n_occur$Freq > 1,]
geoPlaces[geoPlaces$name %in% n_occur$Var1[n_occur$Freq > 1],][,1:2]
# Let's get the placeID of the first one
ID <- geoPlaces[geoPlaces$name %in% n_occur$Var1[n_occur$Freq > 1],][1,1]
ID
backup <- places$name
places$name <- as.character(places$name)
for (i in 1:length(places$placeID)) {
if (places$placeID[i] == ID) {
places$name[i] = "Gorditas Dona Tota 2"
}
}
places$name <- as.factor(places$name)
places_stm <- select(places, placeID, userID, rating, name)
places_stm$rating <- as.numeric(places_stm$rating)
utMx <- with(places_stm, tapply(rating, list(userID, name), sum, default=NA))
dim(utMx)
utMx[1:10, 3:6]  # Here we can observe that it is a disperse matrix
place_names <- colnames(utMx)
head(place_names)
write.csv(utMx, file = "RCdata/Utility_Matrix.csv", na="")
#install.packages("irlba")
library(irlba)
utMx[is.na(utMx)] <- 0
MSVD <- svd(t(utMx))  # in MSVD we have u, d and v that forms from the decomposition
dim(MSVD$u)  # We can observe we end with the right size of u, because we transpose the utility matrix in the line before
placesLatentMatix <- MSVD$u
placesLatentMatix[1:5,1:7]
utrunc <- MSVD$u[,1:10]
corr_mx <- cor( t(utrunc), use='pairwise.complete.obs', method = "pearson")
df_cor <- as.data.frame(corr_mx)
library(ggplot2)
library(reshape2)
tmp <- melt(data.matrix(cor(df_cor)))
ggplot(data=tmp, aes(x=Var1,y=Var2,fill=value)) + geom_tile()
place_names <- colnames(utMx)
# We select the name
name2look <- 'Gorditas Doa Gloria'
#And obtain the index of the correlation matrix
nameIndex <- which(place_names == name2look)
restOptions <- corr_mx[,nameIndex]
# We obtain the index organized from higher correlation to lower correlation
orderedRest <- order(-restOptions)
# And then we use these indices to extract the names associated with those indices with greater correlation
# We ignore the first index since it will be the one with correlation 1, that is, the same restaurant and we will not recommend the same restaurant
numberOfRecomm <- 12
RecommendedRest <- place_names[orderedRest[2:numberOfRecomm+1]]
for (i in 1:11){
print(RecommendedRest[i])
}   # Here are the 12 recommended restaurants
plot(MSVD$d)
