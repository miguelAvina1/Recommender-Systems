---
title: "Recommender Systems Homework" 
output: html_notebook
---


## Intelligent systems <img src="Images/tecnologico-de-monterrey-blue.png" alt="Tec Logo" width="200" style="float:right"/>
#### Date: June 3rd, 2020 
<hr>
### Professor:
#### PhD. Luis Eduardo Falcón 
### Authors:
#### Miguel Aviña &nbsp;- &nbsp; A01227898
#### Arturo Perez &nbsp;- &nbsp; A01228405

<hr>

## Index

#### 0. [Importing files]
#### 1. [Global data frame building] 
#### &nbsp;&nbsp;&nbsp;&nbsp;     1.1 [Merging *geoplaces* and *rating* datasets.]
#### &nbsp;&nbsp;&nbsp;&nbsp;     1.2 [Merging *cuisine* with *geoplaces* and *rating*]
#### 2. [Descriptive Analysis]
#### &nbsp;&nbsp;&nbsp;&nbsp;     2.1 ['City' missing data]
#### &nbsp;&nbsp;&nbsp;&nbsp;     2.2 ['RCuisine' missing data]
#### 3. [Dataset cities]
#### 4. [Popular restaurants]
#### &nbsp;&nbsp;&nbsp;&nbsp;     4.1 [Total number of restaurants]
#### 5. [Popular cuisines]
#### &nbsp;&nbsp;&nbsp;&nbsp;     5.1 [Total number of cuisines]
#### 6. [Utility matrix]
#### &nbsp;&nbsp;&nbsp;&nbsp;     6.1 [Latent variable matrix (Restaurants)]
#### 7. [Recommendation with Pearson correlation]
#### 8. [Conclusion]

<hr>

## Importing files

<span style="color:#0834a4; font-family:Georgia; font-size:1.3em;">
 Use the dataset from the UCI Irvine Machine Learning Repository available for download [here](https://archive.ics.uci.edu/ml/datasets/Restaurant+%26+consumer+data) and utilize the following files and variables:
</span>
<span style="color:#0834a4; font-family:Georgia; font-size:1.2em;">
<br>
&nbsp; - <b>rating_final.csv</b>: Use only variables <b>userID, placeID and rating</b>
<br>
&nbsp; - <b>geoplaces2.csv</b>: Use only variables <b>placeID, name, city, latitude and longitude</b>
<br>
&nbsp; - <b>chefmozcuisine.csv</b>: Use only variables <b>placeID and RCuisine</b>
</span>
<br><br>

We will start by importing the three previously mentioned files:
```{r}
geoPlaces <- read.csv(file = "RCdata/geoplaces2.csv", header = TRUE, sep = ',')
rating    <- read.csv(file = "RCdata/rating_final.csv", header = TRUE, sep = ',')
cuisine   <- read.csv(file = "RCdata/chefmozcuisine.csv", header = TRUE, sep = ',')
```

1. **chefmozcuisine.csv:** 
    a. Instances: 916
    b. Attributes: 2
<br><br>
1. **geoplaces2.csv:**
    a. Instances: 130
    b. Attributes: 21
<br><br>
1. **rating_final.csv:**
    a. Instances: 1161
    b. Attributes: 5

Please refer to the complete [description](RCdata/README) for more information regarding each particular attribute.
<hr>
Then, we will fileter the variables we need.
```{r}
library(dplyr)
geoPlaces <- select(.data = geoPlaces, placeID, name, city, latitude, longitude)
rating    <- select(.data = rating, userID, placeID, rating)
cuisine   <- select(.data = cuisine, placeID, Rcuisine)

```
From now on, we will be using the names *geoPlaces*, *rating* and *cuisine* to refer to the dataframes we have created.

1. **cuisine:** 
    a. Instances: 916
    b. Attributes: 2
<br><br>
1. **geoPlaces:**
    a. Instances: 130
    b. Attributes: 21
<br><br>
1. **rating:**
    a. Instances: 1161
    b. Attributes: 5
    
<hr>
By displaying the first entries of each dataframe we obtained a general idea of the data.
```{r}
head(geoPlaces)
```
<center>**Figure 1. First entries of geoPlaces**</center>
```{r}
head(rating)
```
<center>**Figure 2. First entries of rating**</center>
```{r}
head(cuisine)
```
<center>**Figure 3. First entries of cuisine**</center>
<br>
<br>
We noticed the following aspects:
<br>

1. Some entries of the 'city' attribute refers to the same city but with different notation. <br>
    *E.g:* "s.l.p" and "San Luis Potosi". We might consider removing this attribute or
    unifying these different notations into a single name for each city.
    
1. 'name' attribute is listed in a natural human-readble way.
    Further analysis using this kind of format could be problematic. We might consider
    removing this attribute and kept only with placeID.
<hr>

## Global data frame building
<span style="color:#0834a4; font-family:Georgia; font-size:1.3em;">
From the imported files, generate a single data frame with the previously indicated variables
</span>

Before performing a merge between the three dataframe we have so far, we observed that there are 
different number of observations in each one of the them:
```{r}
sprintf("Number of Observations in cuisine: %s",dim(cuisine)[1])
sprintf("Number of Observations in geoPlaces: %s",dim(geoPlaces)[1])
sprintf("Number of Observations in rating: %s",dim(rating)[1])
```
<br>
In *rating* we expect to have repeated observations (or entries) for both UserID and placeID, because each user could have rated more than one place, and, similarly, more than one place could have had many ratings (score) from different users.
<br>
<br>
For clarity, we will extract the number of **unique** userID and placeID, which will tell us **how many different users and restaurants are in the dataset**.
```{r}
rating_UserIDs <- unique(rating$userID)
sprintf("Number of unique userID: %s", nlevels(rating_UserIDs))
```
<br>
From this point, we know that we are handling score data from **138 different users**. The UserID increments from U1001 to U1138.
```{r}
rating$placeID <- as.factor(rating$placeID)
sprintf("Number of unique placeID: %s",nlevels(rating$placeID))
```
<br>
We have also found that there are **130 different restaurants**. The ID does not increase in an ordered way, so there are gaps in the PlaceID. This indeed correspond to the number of observations of *geoPlaces* we displayed before.
<br>
<br>
RStudio provides an easy way of visualizing the data we are handling. For each data frame, it displays each one of their variables including its type and the first entries. By using this graphical interface (Fig. 4), we noticed that there are 129 levels of the variable "name" in the *geoPlaces*, but we expect 130, one for each placeID. This might mean that there is a repeated name. We will analyze the data to confirm this hypothesis.<br><br>

<div style="text-align: center">
<img src="Images/geoPlacesDataFrame.PNG" alt="R Studio GUI" width="700" />
</div>
<center>**Figure 4. RStudio data frame visualization**</center>

```{r}
n_occur <- data.frame(table((geoPlaces$name)))
colnames(n_occur) <- c("Restaurant_name", "Freq")
n_occur[n_occur$Freq > 1,]
```
<center>**Figure 5. Repeated restaurant**</center>
<br>
<br>
We could see that Gorditas Dona Tota is repeated, this might not be interesting but it is good to know to avoid any misinterpretation in the future.
```{r}
geoPlaces[geoPlaces$name %in% n_occur$Restaurant_name[n_occur$Freq > 1],]
```
<center>**Figure 6. Information about the repeated restaurant**</center>
<br>
<br>
Finally, we will analize the data in the *cuisine* dataframe. What we would expect here is to have the same 130 levels for placeID as in *rating* and *geoPlaces* dataframes. As we see in Fig. 7, the variable "Rcuisine" is a 59 level factor, which tell us that there are 59 different cuisines in the dataset.
We can also observe that we have 916 observations in *cuisine* dataframe while we are only dealing with 130 restaurants. 
<br>

<div style="text-align: center">
<img src="Images/cuisineDataFrame.PNG" alt="R Studio GUI" width="700" />
</div>

<center>**Figure 7. RStudio cuisine data frame visualization**</center>

```{r}
cuisine$placeID <- as.factor(cuisine$placeID)
sprintf("Number of different placeID: %s",nlevels(cuisine$placeID))

```
<br>
By converting placeID to factor, we noticed that there are 769 different placeID's. This means that the *cuisine* dataframe has information of restaurants that we don't have in the other two dataframes. By doing the intersection between rating\$placeID and cuisine\$placeID, we can know which restaurants (placeIDs) they have in common.
```{r}
commonPlaceIDs <- as.factor(intersect(cuisine$placeID, rating$placeID))
sprintf("Intersection between rating and cuisine: %s",nlevels(commonPlaceIDs))
```
<br>
We were expecting to have an intersection of 130; it would have mean that *cuisine* dataframe has the cuisine information for all the places we have in the other two dataframes. However, we found that they only intersect in 95 places, meaning that **we don't have cuisine information of 35 restaurants**.
Further, since *cuisine* have 916 observations but only 769 placeID's, we could conclude that **there are more than one cuisine assigned to each placeID**.

We will filter the repeated placeID's with more than one occurence in *cuisine* in order to get a better understanding of this data frame. Below we display the most repeated placesID's.
```{r}
n_occur <- data.frame(table(cuisine$placeID))
n_occur <- n_occur[order(-n_occur$Freq),]
colnames(n_occur) <- c("placeID", "Freq")
head(n_occur[n_occur$Freq > 1,])
```
<center>**Figure 8. Frequencies of most repeated restaurants**</center>
<br>
<br>
The table before showed us that there are restaurants that appears more than two times, even nine times in the *cuisine* dataframe. We will have a look of some of the repeated entries in the dataset:
```{r}
cuisine[cuisine$placeID %in% n_occur$placeID[n_occur$Freq > 1],][1:10,]
```
<center>**Figure 9. Repeated entries in cuisine dataframe**</center>
<br>
From the previous table we now have an understanding on how the data is listed. We observe that there's a row per each cuisine a restaurant has.
<br>
Let's have a look of the most repeated restaurant in *cuisine*:
```{r}
filter(cuisine, placeID == n_occur$placeID[1])  
```
<center>**Figure 10. Most repeated placeID in cuisine**</center>
<br>
After this analysis of the three imported data sets, we have discovered the following:
<br>

1. *rating* dataframe and *geoPlaces* dataframe have the same set (130) of restaurants (PlaceIDs), as we would expect.

1. *cuisine* dataframe has a larger set of restaurants (769) than *rating* and *geoPlaces*

1. Not all restaurants listed in *rating* and *geoPlaces* dataframes are in *cuisine*: **we have restaurants from which we don't know their cuisine**

1. In *cuisine*, some placeIDs have more than one cuisine linked to them.
<br>
<div style="text-align: right"> Return to [Index] </div>

### Merging *geoplaces* and *rating* datasets.

In order to create the global dataframe needed (recall that, to have a dataframe, we need the same number of observation in each variable) we will start by merging *geoPlaces* and *rating* using the variable "placeID' as pivot. This would add the "name", "city", "latitude" and "longitude" information for each placeID. We will perform this using the merge() function without any preprocessing of the data in the two dataframes.
Notice that, we have 130 observations in *geoPlaces* and 1161 observations in *rating* so we will expect a merged dataframe with 1161 observations.
<br>
```{r}
places <- merge(rating, geoPlaces, by="placeID")
sprintf("Number of observations in merged dataframe: %s",dim(places)[1])
```
<br>
<div style="text-align: right"> Return to [Index] </div>

### Merging *cuisine* with *geoplaces* and *rating*
We can not merge the *cuisine* dataframe directly as we have done in the previous section. Doing it this way would result in:

1. Dropping all the non-useful placeIDs from *cuisine*. The ones that are not part of the 130 placeIDs of *rating* and *geoplaces*, and now in the new dataframe *places*.

1. **Extra 'votes' (ratings) will be added to the merged result because *cuisine* have more than one entry per placeID.**

<br>
Doing this will alter the information and will result in wrong results when getting the [Popular Restaurants] and, in general, to all the analysis beyond this point.
<br>
<br>
However, we will demonstrate what would happen if we merge *cuisine* directly without any modification.
<br>
We will get one placeID to focus on. We will choose a PlaceID that exists in both *rating* and *cuisine* dataset and that is repeated (have more than one cuisine) in *cuisine*.
<br><br>
Getting all placeIDs' with more than one cuisine assigned to them.
```{r}
z <-  cuisine[cuisine$placeID %in% n_occur$placeID[n_occur$Freq > 1],]
```
<br><br>
Extracting the first observation for reference
```{r}
examplePlaceID <- intersect(commonPlaceIDs, z$placeID)[1]
```
<br><br>
As shown in Fig. 11, this particular placeID has 10 votes (ratings) made by 10 different users.
```{r}
filter(rating, placeID == examplePlaceID)
```
<center>**Figure 11. *rating* subset: PlaceID 135086 votes.**</center>
<br>
<br>
Furthermore, as shown in Fig. 12, the user U1108 has voted for this placeID 135086 once, giving the place a rating of '1'.
```{r}
examplePair <- filter(rating, placeID == examplePlaceID)[1,1:3]
exampleUserID <- (examplePair[1])
examplePair
```
<center>**Figure 12. Example of specific userID-placeID**</center>
<br>
<br>
Now, we will merge *cuisine* with *places* (Recall that *places* is the merged dataframe of *rating* and *geoPlaces*) which will result in an inadequate merged dataframe.
```{r}
badMerge <- merge(places, cuisine, by="placeID")
```
<br>
In Fig. 13 we can see the same placeID (135086) in the merged result. Observe how it went from having 10 votes (Fig. 11) to 20 votes.
```{r}
z <- filter(badMerge, placeID == examplePlaceID)
z
```
<center>**Figure 13. Examples of reapeated votes**</center>
<br>
<br>
Inf. Fig. 14 we can see the same UserID-PlaceID record shown in Fig. 12. Observe how this userID has two votes for the same placeID. We conclude that this merge can not be done this way since it will affect the original data and lead to bad analysis results. Particulary, what will happen is that placeID's with *n* cuisine assigned to them will increase its votes and popularity *n* times.
```{r}
filter(z, userID ==  exampleUserID[1,1])
```
<center>**Figure 14. Same userID-placeID as Fig. 12 in the merged dataframe**</center>
<br>
<br>
<br>
Preserving cuisine information without modifying the ratings can be done in many different ways. We will focus on doing what we will need for this homework, however, it might not be the best solution if further analysis need to be done with cuisine information. Our solution will be to merge placeIDs' cuisine information into a single level. Taking the example n Fig. 14, placeID will be combined into:
<br>

<div style="text-align: center">
<img src="Images/ExpectedMerge.PNG" alt="ExpectedMerge" width="1000" />
</div>
<center>**Figure 15. Desired merge for a placeID with Fast_Food and Burgers cuisine information**</center>
<br>
<br>
We are taking this aproach because there are other levels already with this kind of syntax. *E.g.:* Bar_Pub_Brewery.
<br>
<br>
We start by keeping just the placeIDs from *cuisine* we are interested in (the 130 listed in *geoPlaces*):
```{r}
cuisine <- filter(cuisine, placeID %in% commonPlaceIDs)
```
<br>
After this filter, we end up with 112 observations. Now, we will combine this observations into the real 95 unique placeIDs that we have info. After this process, we will have an useful cuisine data for the 130 places we are analyzing (Remember that we will be still having 35 places without cuisine information).
```{r}
merged_Places <- list()
merged_Cuisines <- list()
#cuisine_merged_Places <- c(cuisine_merged_Places, 3)

for (place in commonPlaceIDs) { # We will iterate along each placeID we have 
  currentPlaceID <- filter(cuisine, placeID == place)    # and will examine one by one
  new_level2 <- as.character(currentPlaceID$Rcuisine[1])
  currentPlaceID <- currentPlaceID[-c(1),]
  for (cuisineKind in currentPlaceID$Rcuisine) {
    new_level2 <- paste(new_level2, as.character(cuisineKind), sep='_')
  }
  merged_Places <- c(merged_Places, place)
  merged_Cuisines <- c(merged_Cuisines, new_level2)
}
cuisine <- data.frame('placeID' = unlist(merged_Places, use.names = FALSE), 'cuisine'=unlist(merged_Cuisines, use.names = FALSE) )
rm(merged_Places)
rm(merged_Cuisines)
rm(currentPlaceID)
```
<br>
Finally, we will merge this data with *places*
```{r}
places_drop <- merge(places, cuisine, by="placeID")
uniquePlaceID <- droplevels(as.factor(unique(places_drop$placeID)))
places <- merge(places, cuisine, by="placeID", all=TRUE)
levels(places[,8]) <- c(levels(places[,8]),"?")   # we are adding '?' to tell that we dont have that info
places[is.na(places)] <- '?'
```

Here we can see the first entries of the merged dataset and a summary:
```{r}
head(places)
```
<center>**Figure 16. First entries of the merged data**</center>
<br>
```{r}
summary(places)
```
<center>**Figure 17. Summary of merged dataset**</center>
<br>
<hr>

<div style="text-align: right"> Return to [Index] </div>

### Descriptive Analysis
<span style="color:#0834a4; font-family:Georgia; font-size:1.3em;">
Realize a descriptive analysis of the variables and indicate if there are missing data in any of them. If so, realize an analysis to determine and justify the decision you take regarding those missing data.
</span>

<br>

#### **placeID**

<br>
This variable was numeric and we already changed it to factor because it must be taken as a qualitative variable.
```{r}
sprintf("Number of places in the dataset: %s",nlevels(places$placeID))
```
<br>
In previous homeworks, we analyzed the proportion of observations per level, however, this does not make sense in this homework  since it would not be useful to combine them, furthermore, the analysis we will do later does not benefit from merging levels. Some level's proportions are shown in Fig. 18; however, this data is not very useful.
```{r}
head(prop.table(table(places$placeID)))
```
<center>**Figure 18. Proportions of some placeIDs**</center>
<br>
In Figure 19, we can observe a barplot of the places and their absolute frequencies. 

This kind of graphic is useful, it shows a nice
representation of the number of occurences for each placeID, that is, the popularity of the restaurants. 
<br>

```{r}
barplot(height = table(places$placeID), xlab = "placeID", ylab = "Ratings received")
``` 
<center>**Figure 19. Barplot of number of occurences for each placeID**</center>
<br>
<br>
Graphically, we can also observe that it seem to be a mean value of ~ 5 ratings per place, lets get this number right.
```{r}
library('plyr')
t <- count(places, 'placeID')
sprintf("Average votes per placeID: %s", mean(t$freq))
```
We know that 'placeID' is a categorical variable, however, the number of occurences per each placeID are basically the number of votes per restaurant and this data is numerical so we will analyze it:
```{r}
summary(t$freq)
```
The interpretation we can get from the previous summary is that there's a restaurant with 36 votes and that the restaurant with fewer votes has just three. Using the numerical data from the number of votes per restaurants, we created the boxplot shown in Fig. 20. In the boxplot we can graphically identify some atipical values, these atipical values could be seen and interpreted as outstanding (or very popular) restaurants. We are not going to do anything with this outliers because removing them does not make any sense sice we would just lose important data. 
<br>
```{r}
boxplot(t$freq)
```
<center>**Figure 20. Boxplot of placeID**</center>
<br>

#### **userID**

<br>
'UserID' was already a categorical (factor) variable, so we didn't do any modification.
<br>
We'll do the same analysis for this variable as we did for placeID because they are fair similar variables.
```{r}
barplot(height = table(places$userID), xlab = "userID", ylab = "# of places rated")
```
<center>**Figure 21. Bar plot of number of places an user has rated**</center>
<br>
<br>
In Fig. 21 we can observe the number of times each userID appears in the data, this can be interpreted as the number of places each user has rated. It doesn't show too many uncommon users; altough there are some larger bars, they doesn't stand out as much as the larger bars from placeID in Fig. 19. 
<br>
As we did for placeID we will get the average places rated per user and the summary of the data plotted in Fig. 21.

```{r}
t <- count(places, 'userID')
sprintf("Average places rated per User: %s", mean(t$freq))
```

```{r}
summary(t$freq)
```

```{r}
boxplot(t$freq)
```
<center>**Figure 22. Boxplot of occurences of each userID**</center>
<br>
<br>
In Fig. 22 we observe what we mentioned before: there are not uncommon users. Nobody rated too much or too little to become an outlier.
<br>
<br>

#### **Rating**
<br>
Rating was imported as a numerical variable. If we check the dataframe using RStudio, we can manually observe that this variable has only three values: 0, 1, 2. Turning it into categorical (factor) variable would be better.
<br><br>
However, lets assume we have a significantly larger dataset and manually inspecting the data from the dataframe wouldn't be feasible. Then, we would start by plotting the rating as histogram (Fig. 23). After looking at this representation we would be sure that there are only three levels, so a conversion to factor must be done. Even if we would have decided to use a density plot (Fig. 24) instead of a bar plot, we could have also known that there are only three levels in the data. These examples are provided to demonstrate that it is not necessary to look directly into the observations of the dataframe to know if a variable must be converted to factor.
```{r}
hist(places$rating, xlab = 'Rating', main = 'Histogram of "Rating"')
```
<center>**Figure 23. Histogram of rating**</center>
<br>
<br>
```{r}
plot(density(places$rating), main = "Density plot for 'rating'")
```
<center>**Figure 24. Density plot of rating**</center>
<br>
<br>
<br>
Now we are going to convert this data to a factor.
```{r}
places$rating <- as.factor(places$rating)
```
In the summary shown below, we we right in assuming that there were only three factors. Also, the the number of occurences of each level is shown.
```{r}
summary(factor(places$rating))
```
<br>
Now it makes sense to check the proportion of observations per level. In the table below we can observe that there is a good distribution of observations per level (>10%). This 10% threshold is useful when creating models (as we did for titanic and germancredit). Even if we didn't have this distribution, we would not merge the rating's levels.
```{r}
prop.table(table(places$rating))
```

<br>

#### **Name**

<br>
There are not many options we can do to analyze this variable. Below we are just looking at some of the first observations of this variable.
```{r}
head(places$name)
```
Each place name is correlated to a placeID, therefore, this variable helps to have a better identifier to the numerical placeID. 
<br>
<br>

#### **City**
```{r}
sprintf("Number of cities: %s",nlevels(places$city))
```
```{r}
levels(places$city)
```
<center>**Figure 25. Levels of 'city' variable**</center>
<br>
<br>
After displaying the city's level, it is evident that same places are taken different because of typos or extra spaces. It is important to remark that looking to the all levels is only useful when handling few of them, if we had larger sets with a lot of levels, it wouldn't be possible. 
<br>
Also, in this case we could simply use the latitude and longitude data to retrieve the city and just ignore the original imported city variable.
However, we are going to assume that we do not have latitude nor longitude, therefore, we will merge the levels manually:
<br>
If we had significantly more levels, we may try to get the correlation between level names to identify same cities with typos or written a bit different, however, this is out of the scope of this homework due to time limitations.
```{r}
places$city <- factor(places$city, labels=c("Undefined","Cd. Victoria","Cd. Victoria","Cd. Victoria",
                                            "Cuernavaca","Cuernavaca","Jiutepec", "San Luis Potosi",
                                            "San Luis Potosi", "San Luis Potosi", "San Luis Potosi",
                                            "San Luis Potosi", "San Luis Potosi", "San Luis Potosi",
                                            "Soledad", "Cd. Victoria", "Cd. Victoria"))
```
After manually merging the levels, we end with six levels which are:
```{r}
levels(places$city)
```
<center>**Figure 26. Levels of 'city' variable after merging levels**</center>
```{r}
prop.table(table(places$city))
```
<center>**Figure 27. Proportion of each city level"**</center>
<br>
<br>
From Fig. 27 we observe that most of the places are located in San Luis Potosi and none of the other levels have more than 10% of the total observations.
<br>
Now, we will do the same analysis as we did for placeID and userID.
```{r}
barplot(height = table(places$city), las=2) 
```
<center>**Figure 28. Barplot of 'city' variable**</center>
<br>
<br>
```{r}
t <- count(places, 'city')
summary(t$freq)
```
Getting the summary  is useful to know how many ratings (votes) the most 'popular' city has (834) and that the city with fewest votes only has 17 among all its restaurants.
```{r}
boxplot(t$freq)
```
<center>**Figure 29. Boxplot of cities** </center>
<br>
<br>
The boxplot shown inf Fig. 29 shows an outlier value which correspond to San Luis Potosi, the city with most restaurants and therefore with most votes in the data.
<br><br>

#### **Latitude**

<br>
This is an strictly numerical value, so we can do a numerical analysis. We will get the summary of the data, including its standard deviation. Then we will show the data using an histogram and a density plot. Some insigths based on the results are provided after each step.
```{r}
print(summary(places$latitude))
```
```{r}
sprintf("Std dev: %s", round(sd(places$latitude, na.rm = TRUE), digits = 4))
```
The information of summary and the standard deviation can be particularly helpful for a person with advanced geographical knowledge who could identify the zone in latitude the places are located. 
```{r}
hist(places$latitude)
```
<center>**Figure 30. Histogram of 'latitude' variable**</center>
<br>
<br>
In the histogram shown in Fig. 30 we observe that the latitude is divided in three main zones or degrees. This shows that, among the five cities we are analyzing, at least two of them share a similar latitude. In the Fig. 31, we can observe this division a bit more clear and identify four 'peaks'.
```{r}
plot(density(places$latitude, na.rm = TRUE))
```
<center>**Figure 31. Density plot of 'latitude' variable**</center>
<br>
<br>
<br>

#### **Longitude**

The longitude variable is very similar to latitude, so we will analyze it the same way. Some insigths based on the results are provided after each step.
```{r}
print(summary(places$longitude))
```
```{r}
sprintf("Std dev: %s", round(sd(places$longitude, na.rm = TRUE), digits = 4))
```
<br>
As we stated before, the information of summary and the stanrdad deviation can be helpful for a person with enough geographical knowledge to interpret the numbers into spacial zones or locations.
<br>
In the histogram shown in Fig. 32 we observe that the longitude is divided in four main zones or degrees. This shows that, among the five cities we are analyzing, one of them share a similar longitude to another. This information and the histogram in Fig. 30 can lead to a conclusion that at least two cities are adjacent.
<br>
```{r}
hist(places$longitude)
```
**Figure 32. Histogram of the 'longitude' variable**
<br>
<br>
```{r}
plot(density(places$longitude, na.rm = TRUE))
```
<center>**Figure 33. Density plot of longitude**</center>
<br>
<br>
<br>
The separated analysis of latitude and longitude variables did not give a lot of significant or helpful information. Normally these data are used together as a set of coordinates, therefore, we will analyze them together as a pairs.

In Figure 33 we plot the longitude vs latitude which generates a virtual map of the restaurants' location.
We observe that there are three main cumulus but we have five different cities. For analayzing data, there must always be context of what we are studying, the cities in the data are from Mexico; in particular, Cuernavaca is a city which adjoints Juitepec that is why there are shown as a single cumulous of points. 
Soledad (Soledad de Graciano Sanchez) is a city which is adjacent to San Luis Potosi, and that is why those two cities merges in one cumulous.
This kind of analysis is more suitable adn helpful for this specific information rather than getting quartiles, mean, standard deviation, etc.
```{r}
plot(places$longitude, places$latitude)
```
**"Figure 34.Graph of latitude and longitude"**
<br>
<br>

#### **Cuisine**

<br>
This variable was imported as a categorical (factor) variable, so we didn't do any modification to it. We will perfom a similar analysis as we did for placeID and UserID.
```{r}
barplot(height = table(places$cuisine), las=2)
```
**"Figure 35.Graphic of cuisine"**
<br>
<br>
```{r}
t <- count(places, 'cuisine')
sprintf("Average places per cuisine: %s", mean(t$freq))
summary(t$freq)
```
In Fig. 36 we observe the boxplot of the occurences for each cuisine. There are only two outliers; one of them must be the missing data ('?') but it would be intersting to know the other. Below we obtain this information.
```{r}
outvals <- boxplot(t$freq)$out
```
**"Figure 36. Boxplot of cuisine"**
```{r}
t[which(t$freq %in% outvals),]
```

**Figure 37. Kind of cuisine of the outliers in Fig. 36**
<br>
<br>
<hr>
<div style="text-align: right"> Return to [Index] </div>

### 'City' missing data

<span style="color:#0834a4; font-family:Georgia; font-size:1.3em;">
Perform an analysis to determine and justify the decision you make about such missing data. <br>
For the case of missing data in the 'city' variable, you could research about at least the packages "revgeo" or "ggmap" from R to retreive the city name using latitude and longitude information.
</span>

<br>
We have missing data in *city* (which came with the dataset since we imported it) and missing data in *cuisine*, that we added when merging cuisine data.

<br>
For the 'city' variable, we will use *revgeo* and *ggmap* to get the city name using latitude and longitude.
```{r}
#install.packages("revgeo")
#install.packages("ggmap")
library(revgeo)
library(ggmap)
```
First we will get the cities' names using *revgeo*.
```{r}
undef_cities <- filter(places, city == 'Undefined') 
undef_cities <- select(.data = undef_cities, placeID, city, latitude, longitude)
```
Here we store in 'undef_cities' a copy of those places without city name information, we will use it later.
<br>
R doesn't allow to add new entries to a factor if they don't belong to the currently defined levels; therefore, we wil un-factorize 'city', then add the new cities we get using *revgeo*, and then factorize the variable again.
Notice that *revgeo* returns a single character strings in the form of:  <br><br>
"House Number Not Found Street Not Found, Cuernavaca, Morelos, 62000, Mexico" <br><br>
And it mantains this specific format for any given coordinates. To extract the city name from this character string we will split the string using ', ' as separator and store the information after the first comma.
```{r}
places$city <- as.character(places$city)
for (i in 1:length(places$city)) {
  if (places$city[i] == 'Undefined') {
    possibleCity <- revgeo(places$longitude[i], places$latitude[i])
    places$city[i] =  unlist(strsplit(as.character(possibleCity), ", "))[2]
  }
}

```
<center>**Figure 38. Getting data from revgeo**</center>

After this, we will re-factorize the variable and show the levels we have after retriving city names with *revgeo*. 
```{r}
places$city <- as.factor(places$city)
sprintf("Number of places: %s",nlevels(places$city))  # 
levels(places$city)
```
**Figure 39. Levels for the 'city' variable after filling missing data with *revgeo* **

We observe in Fig. 39 that we have same cities with different labels, we we will correct them as we did before.
```{r}
places$city <- factor(places$city, labels=c("Cd. Victoria","Cd. Victoria","Cuernavaca","Jiutepec",
                                            "San Luis Potosi", "San Luis Potosi", "Soledad", "Soledad"))
```
```{r}
places$city <- droplevels(places$city)
sprintf("Number of places: %s",nlevels(places$city))  # 
levels(places$city)
```
<center>**Figure 40. Correced vevels for the 'city' variable without missing data**</center>
<br>
<br>
Now we will use *ggmap* package from Google to double check the cities we retrieved using *revgeo*. In theory, we should get the same city name if we use the same longitude and latitude information.
<br>
To use Google's ggmap, it requires an API key so we must register into Google Cloud services to do so:
<br>
First, we had to go to https://cloud.google.com/maps-platform/ and register for a free trial. This trial gave us 300 USD to start using their services which is enough for doing few requests as we will do.
<br>
Then we must enable the google APIs for maps which will generate an API key that we will use to get the required information. 
<br>
Once we have the private key, we register it using ggmap:
```{r}
register_google(key = "AIzaSyD4mKmiXYdtE9xALBocEVu_Tl15a4iEW4I")
```

This key must be kept private and not shared because Google asks for billing credit card when creating a new account, so anyone could take advantage of this key to gain access to free Google cloud services. We recommend to secure your private key by allowing it to be used only from a certain IPs.

In the loop below we execute this Google's API to find city information of all the 'undef_cities' we saved early.
What this loop does is:

1. Iterate trough all observations in the 'city' variable.

1. IF the observation is in undef_cities, the algorithm continue, otherwise it passes to the next city.

1. Execute revgeocode() function to retrieve Google's data.

- This function returns a big payload with a not static structure, that is, it returns a lists of lists which **differs in sizes for each request**. 
Therefore, we can not know before-hand where the city name will be stored in this huge payload. To overcome this problem, we are performing a **depth-first search** in the returned payload. <br>
  To know if we find the same city name returned by *revgeo* in Google's paylod we are:
  1. Changing the city name to lowercase
  2. Removing all characters that are not in a-z set.
  3. Verifying if there exist a subset of the city name in any of the data entries from google. (This let us identify 'Soledad de Graciano Sánchez' as the same of 'Soledad').<br><br>
  
  If we find this subset in google's payload, we save it in a new column named GG_Result, otherwise, we save 'Not the same' . Then we exit the loop and continue with the next city
  
  At the end of the loop we will have a new column named GG_result with three possible levels:
  1. 'Not compared' for those cities that had city name information when imported from the dataset.
  2. A name of the city, for those cities that google returned the same city info as *revgeo*
  3. 'Not the same' for those cities that google returned a different city name than *revgeo*

```{r}
places$GG_result <-  'Not compared'
for (i in 1:length(places$city)) {
  #
  # SO, FOR GG MAP, GGMAP returns a list with all posible addresses it found. However the structure is NOT the same for each
  # address it returns. So, we can not search of a given element inside the lists of lists it returns. We would need to use another 
  # approach. One non-efficient but useful approach would be to loop inside all the info we get when calling revgeocode to search
  # for any coincidence to the data we retrieved from RevGeo, if we find it, then we can be sure we get the correct city
  # using revgeo. Otherwise, we would need to do a manual inspection of the data received from revgeocode
  # The search would be basically a deep-first search
  if (places$placeID[i] %in% undef_cities$placeID) {   # We are just going to comprobate with those cities that were undefined before
    google_result <- revgeocode(c(places$longitude[i], places$latitude[i]), output = "all")$results
    found <- FALSE
    citynameOriginal <- places$city[i]
    compare_city <- as.character(lapply(as.character(citynameOriginal), tolower))
    compare_city <- gsub("[^a-z]","",compare_city)
    # print(compare_city)
    #sprintf("Comparing with '%s'", compare_city)
    for (j in 1:length(google_result)) {
      address_comp <- google_result[[j]]$address_components
      for (k in 1:length(address_comp)) {
        long_name <- as.character(lapply(as.character(address_comp[[k]]$long_name), tolower))
        long_name <- gsub("[^a-z]","",long_name) 
        short_name <- as.character(lapply(as.character(address_comp[[k]]$short_name), tolower))
        short_name <- gsub("[^a-z]","",short_name)
        # We are not going to compae if the compare_city is at least substring of the google's result
        # We are not comparing it letter to letter because we found that google returns 'Soledad de Graciano Sanchez'
        # instead of just 'Soledad', which is the name of the city we got from revgeo. Therefore, comparing it using the
        # '==' operator will return FALSE, and it should be true, So we are going to use grep to check if one is substring
        # of the other
        if (grepl(compare_city, long_name, fixed = TRUE) || grepl(compare_city, short_name, fixed = TRUE)) {
          # print("FOUND:")
          # print(long_name)
          found = TRUE
          break
        }
      }
      if (found == TRUE) {
        break
      }
    }
    if (found == TRUE) {
      places$GG_result[i] = as.character(citynameOriginal)  # Just put the same city 
    } else {
      places$GG_result[i] <- 'Not the same'  # FUrther analysis will be needed
    }
  }
}
```
Now we will check if there is any result fom Google's *ggmap* that is different from what we got from *revgeo*; if so, we are going to do something to choose one of the different results. 
<br>
For this, we will filter out all the rows that have the string 'Not the same' in the GG_result column.
```{r}
differentResults <- filter(.data = places, GG_result == 'Not the same')
```
Now, we will display all the cases where we didn't get the same from google *ggmap* and *revgeo*
```{r}
differentResults <- select(differentResults, -userID, -rating)
differentResults <- unique(differentResults)
differentResults
```

As shown above, we didn't get any result different from Google's *ggmap* so we can continue with our analysis.
<br>

<div style="text-align: right"> Return to [Index] </div>

### 'RCuisine' missing data

<br>
We could have ended without missing data in this variable if we haven't added the *all=TRUE* parameter in the instruction *merge(places, cuisine, by="placeID", all=TRUE)* when merging the cuisine information into the global dataframe, however, we would have ended with fewer observations, losing important information.

We are not going to do anything regarding the cuisine's missing data because there is no way of knowing this information but researching on internet.
This is out of our scope because: 

1. It is not needed for further analysis

1. Time contraints

1. This issue will not case any NA-related problem since we already replaced missing data with character '?'

<br>
<hr>
<div style="text-align: right"> Return to [Index] </div>

### Dataset cities

<span style="color:#0834a4; font-family:Georgia; font-size:1.3em;">
What cities are the study restaurants from?
</span>

<br>
To obtain this information, we just need to refer to the variable 'city' and look at the number of levels and each level's name:
```{r}
sprintf("There are: %s cities",nlevels(places$city))
levels(places$city) 
```
<br>
<hr>
<div style="text-align: right"> Return to [Index] </div>

### Popular restaurants


<span style="color:#0834a4; font-family:Georgia; font-size:1.3em;">
If we consider the popularity of a place as those with the highest number of user reviews (regardless of whether the review was positive or negative), get the names of the 10 most rated / popular restaurants.
</span>

<br>
To obtain the most popular restaurants, we just need to get the number of occurencies (observations) per placeID, since there is one observation per vote (rating). See results in Fig. 41
```{r}
t <- count(places, 'placeID')  # Getting frequency of each placeID
t <- merge(t, places, by='placeID')

t <- select(t, placeID, freq, name)
t <- unique(x = t)


t <- t[order(-t$freq),]  # Ordering in descending order
popularPlaceID <- t$placeID[1:10]  # Here we have the 10 most popular places ID

mostPopular <- subset(places, placeID %in% popularPlaceID) # Here we extract the entire row where the 
# most popular placeID appears. Now we just want 1 row per name

mostPopular <- droplevels(unique(mostPopular$name))
mostPopular <- data.frame(popularPlaceID, mostPopular, t$freq[1:10])
colnames(mostPopular) <- c("placeID", "Restaurant", "Votes")
mostPopular
```
<center>**Figure 41. Top 10 rated restaurants**</center>
<br>
<br>
**As an extra part, we will get those 'best rated' restaurants, that is, the restaurants with more number of '2' as rating.** See results in Fig. 42
```{r}
best_rated <- filter(places, rating == "2")
best_rated <- select(.data = best_rated, placeID, name)
t <- count(best_rated, 'placeID')  # Getting frequency of each best-rated placeID
t <- t[order(-t$freq),] 

bestRatedPlaceID <- t$placeID[1:10]  # Here we have the 10 most popular and best rated places ID
bestRated <- subset(best_rated, placeID %in% bestRatedPlaceID) # Here we extract the entire row of top 10 best rated places
bestRated <- droplevels(unique(bestRated$name))
bestRated <- data.frame(popularPlaceID, bestRated, t$freq[1:10])
colnames(bestRated) <- c("placeID", "Restaurant", "Positive Votes")
bestRated
```
<center>**Figure 42. Best Rated Restaurants**</center>
<br>

We observed that there are some best-rated places that does not fall into the top 10 most popular restaurants, such as 'El Rincon de San Franciso' which has 14 votes of '2'. If we were to create a more complex recommender system, we might opt to use the best-rated restaurants instead of the most-popular restaurants to solve the cold-start problem.
<br>
<br>
<div style="text-align: right"> Return to [Index] </div>

### Total number of restaurants
<span style="color:#0834a4; font-family:Georgia; font-size:1.3em;">
How many different restaurants are there in total?
</span>

To get this information we just need to check the number of levels in placeID as follows:
```{r}
nRestaurants <- nlevels(places$placeID)
sprintf("In this data set we have a total of %s different restaurants", nRestaurants)
```

<br><br>
<div style="text-align: right"> Return to [Index] </div>

### Popular cuisines

<span style="color:#0834a4; font-family:Georgia; font-size:1.3em;">
What type of food / cuisine are the 10 most popular restaurants found in the previous point?
</span>

<br>
<br>
Here we fill the 'mostPopular' with the cuisine information using placeID as pivot.
```{r}
mostPopular <- merge(mostPopular, cuisine, by='placeID')
mostPopular
```
<center>**Figure 43. Most popular restaurants and their kind of food (cuisine)**</center>
<br>
<br>
Then we will just extract the cuisine information as follows:
```{r}
levels(droplevels(mostPopular$cuisine))
```
<center>**Figure 44. Most popular cuisines**</center>
<br>
<br>
<div style="text-align: right"> Return to [Index] </div>

### Total number of cuisines
<span style="color:#0834a4; font-family:Georgia; font-size:1.3em;">
How many different types of cuisine are there in total?</span>
<br>
Here we can have three different results:
<br>
<br>
a. Different types of food inside the dataset of the 130 places that are **already merged.**
```{r}
sprintf("Number of types of cuisines: %s",nlevels(droplevels(places$cuisine)))
levels(places$cuisine)
```
<br><br>
<br>
b. Different types of cuisines inside the dataset that was imported, **without processing.** Since we have made a lot of modifications, we will import the dataset again for simplicity. Recall that we have this many cuisine types because in the original dataset we had extra placesID we are not analyzing in the other sets.

```{r}
cuisine_original   <- read.csv(file = "RCdata/chefmozcuisine.csv", header = TRUE, sep = ',')
cuisine_original   <- select(.data = cuisine_original, placeID, Rcuisine)
sprintf("Number of types of cuisine: %s",nlevels(cuisine_original$Rcuisine))
```

<br><br>
c. Types of cuisine inside the dataset of **original cuisine**, just the 130 places that we analyzed.

```{r}
commonPlaceIDs <- as.factor(intersect(cuisine$placeID, rating$placeID))
cuisine_original_130 <- droplevels(subset(cuisine_original, placeID %in% commonPlaceIDs))
sprintf("Number of types of cuisine: %s", nlevels(cuisine_original_130$Rcuisine))
levels(cuisine_original_130$Rcuisine)
```

<br>
<br>
<hr>
<div style="text-align: right"> Return to [Index] </div>

### Utility matrix

<span style="color:#0834a4; font-family:Georgia; font-size:1.3em;">
Generate the utility matrix considering the rows with the variable userID, the columns with placeID, and the values of the matrix with rating. 
</span>

<br>
First, we will get the variables we will need in a dataframe in a simple-triple-matrix form.
```{r}
places_stm <- select(places, placeID, userID, rating, name)
head(places_stm)
```
<center>**Figure 45. First entries of the dataset with the variables needed for to generate an utility matrix**</center>
<br>
<br>
Altough we used rating as a categorical value previously, here we must convert to numerical variable for this analysis.
```{r}
places_stm$rating <- as.numeric(places_stm$rating)
utMx <- with(places_stm, tapply(rating, list(userID, name), sum, default=NA))
sprintf("Dimension of the matrix: %s",dim(utMx))
```
We can observe that we created a matrix of 138x129, 138 correspond to the users we know we have but we should have 130 restaurant names, where is the missing one?
<br>
If we recall, in our first analysis **we identified two places with the same name**. This characteristic hasn't been problematic until know. We will change one of those places' name so we can identify it as separate restaurants.
<br>
We will retrieve the repeated name again
```{r}
n_occur <- data.frame(table((geoPlaces$name)))
n_occur[n_occur$Freq > 1,]
```
<center>**Figure 46. Repeated restaurant name**</center>
<br>
<br>
```{r}
geoPlaces[geoPlaces$name %in% n_occur$Var1[n_occur$Freq > 1],][,1:2]
```
<center>**Figure 47. Repeated names and their placeID**</center>

<br>
<br>
We will get the placeID of one of the repeated name.
```{r}
# Let's get the placeID of the first one
ID <- geoPlaces[geoPlaces$name %in% n_occur$Var1[n_occur$Freq > 1],][1,1]
ID
```
<center>**Figure 48. placeID of one of the repeated restaurants**</center>
<br>
<br>
Now, we will replace "Gorditas Dona Tota" to "Gorditas Dona Tota 2". 
<br>
```{r}
backup <- places$name
places$name <- as.character(places$name)
for (i in 1:length(places$placeID)) {
  if (places$placeID[i] == ID) {
     places$name[i] = "Gorditas Dona Tota 2"
  }
}
places$name <- as.factor(places$name)
```


After this modification, we will repeat the process of creating the utility matrix again. We observe that we end with the correct dimensions of the matrix: 138 x 130. In Fig. 49 we show a section of utility matrix, observe that it is a disperse matrix since most of the data is 'NA'. 
```{r}
places_stm <- select(places, placeID, userID, rating, name)
places_stm$rating <- as.numeric(places_stm$rating)
utMx <- with(places_stm, tapply(rating, list(userID, name), sum, default=NA))
dim(utMx)
```
```{r}
utMx[1:10, 3:6]  # Here we can observe that it is a disperse matrix
```
<center>**Figure 49. Utility matrix of the restaurants and userIDs**</center>
<br>
<br>
We will store the columns' names which correspond to the restaurants' names so we can retrive this data later. Finally, we will save this utility matrix in a .csv.

```{r}
place_names <- colnames(utMx)
head(place_names)
```

```{r}
write.csv(utMx, file = "RCdata/Utility_Matrix.csv", na="")
```
<div style="text-align: right"> Return to [Index] </div>

### Latent variable matrix (Restaurants)

<span style="color:#0834a4; font-family:Georgia; font-size:1.3em;">
From this utility matrix, apply SVD (Singular Value Decomposition) factorization to obtain the latent variable matrix for restaurants.
</span>

```{r}
#install.packages("irlba")
library(irlba)
```

If we want to generate the matrix of latent variables for restaurants, we should end with a matrix 'u' with dimensions 130x130
<br>
First we will change the "NA" values to 0 to avoid any problems while executing the decomposition function svd(). Then we will save the output of the svd() function into the variabel MSVD. What the svd function returns are the matrices 'u', 'v' and the singular value vector 'd'.
```{r}
utMx[is.na(utMx)] <- 0
MSVD <- svd(t(utMx))  # in MSVD we have u, d and v that forms from the decomposition
dim(MSVD$u)  # We can observe we end with the right size of u, because we transpose the utility matrix in the line before
```

The latent variable matrix for the places corresponds to the 'u' matrix of the decomposition. Below we are just showing some entries of this matrix.
``` {r}
placesLatentMatix <- MSVD$u
placesLatentMatix[1:5,1:7] 
```
<center>**Figure 50. Some entries of the latent matrix for restaurants**</center>

<br>
<div style="text-align: right"> Return to [Index] </div>

### Recommendation with Pearson correlation

<span style="color:#0834a4; font-family:Georgia; font-size:1.3em;">
If a user liked the restaurant “Gorditas Doña Gloria” with placeID: 132834, what other 12 restaurants (indicate the names) could you recommend using the SVD decomposition with the "pearson" correlation method and considering the 10 largest singular values of the SVD? Note: it doesn't matter for the moment what city be the restaurant.
</span>

First we will truncate the matrix 'u', or the latent variable matrix, to the first 10 latent columns, corresponding to the **10 largest singular values**. We can perform a simple truncation since the svd () function we used before returns the singular values in the 'd' vector ordered from highest to lowest.
```{r}
utrunc <- MSVD$u[,1:10]
```

Now, we will get a correlation matrix using the truncated SVD matrix, using the 'Pearson correlation'. Then we will store this matrix in 'corr_mx'. In Fig. 51, we show the correlation matrix using the *ggplot* matrix. However, since the matrix is large, we can not get much insights by generating this graphical repesentation.
```{r}
corr_mx <- cor( t(utrunc), use='pairwise.complete.obs', method = "pearson")
```

```{r}
df_cor <- as.data.frame(corr_mx)
library(ggplot2)
library(reshape2)

tmp <- melt(data.matrix(cor(df_cor)))
ggplot(data=tmp, aes(x=Var1,y=Var2,fill=value)) + geom_tile()
```
<center>**Figure 51. Graph of correlation matrix**</center>
<br>
<br>

```{r}
place_names <- colnames(utMx)
```
Now, from a restaurant name, lets get the index position 'Gorditas Dona Gloria' in the corelation matrix:
```{r}
# We select the name
name2look <- 'Gorditas Doa Gloria'
#And obtain the index of the correlation matrix
nameIndex <- which(place_names == name2look)
```
Now we will select the column of this restaurant from the correlation matrix. After this, we arrange in descending order the column so we will keep with the higher correlated restaurants with 'Gorditas Dona Gloria' at the top of the column. Then, we will truncate this column to the first 12 rows which corresponds to the 12 restaurants we will recommend. These restaurants are shown below.
```{r}
restOptions <- corr_mx[,nameIndex]
# We obtain the index organized from higher correlation to lower correlation
orderedRest <- order(-restOptions)
# And then we use these indices to extract the names associated with those indices with greater correlation
# We ignore the first index since it will be the one with correlation 1, that is, the same restaurant and we will not recommend the same restaurant
numberOfRecomm <- 12  
RecommendedRest <- place_names[orderedRest[2:numberOfRecomm+1]]

for (i in 1:11){
  print(RecommendedRest[i])
}   # Here are the 12 recommended restaurants

```
<div style="text-align: right"> Return to [Index] </div>

### Conclusion

Altough this homework was intended to exercise our knowledge regarding recommender systems, we faced some other situations that may occur while analyzing data from different datasets. For example, we showed the importance of analyzing the data before merging it into a dataframe. If we hadn't done this, we would have ended with very different results in many of the homework's points.

Also, we learned how to get a bit more of information of categorical variables by getting the frequency of occurence of each level, which gives use numerical information we can analyze, as we showed for several factors. Furthermore, we got familiar with cloud services such as Google's places service that we used. 

It is also important to notice the context of the data, while analyzing the latitude and longitude of the cities, while we had five cities, there were just three groups in the plot, it is important to understand the two of the cities were adjacents to two of the other cities.

Regarding the decomposition by svd, we truncated the output of the svd() function to the first 10 values, this is indeed a way to balance the complexity of the model mantaining its accuracy. However, we thought that cutting it to the first 10 values, that is, the first 10 vectos of the latent matrix 'u' which are related to the 10 greatest singular values of the vector 'd' is not the best approach. We have lost some infromation due to this truncation, but **how are we sure we truncated it to the right length to avoid missing too much information?**
<br>

There are several way of calculating this truncation value, most of them are empirical, such as plotting the singular values and locating the 'elbow' of the graph:
```{r}
plot(MSVD$d)
```
<center>**Figure 52. Graph of singular values**</center>
<br>
In Fig. 52 we observe that the graph doesn't show an evident 'elbow', so we couldn't take this approach.
For this homework we kept with the first 10 value truncation, mainly because of the time constraint. However, as a future work we would like to implement the Gavish and Donoho approach they discribe in the paper [The Optimal Hard Threshold for Singular Values is 4/√3](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6846297)  **that computes what is the best truncation value** to avoid missing too much information yet reducing the complexity of carrying the complete latent variable matrix. 
<br>
As the name of the paper states, this truncation value is **4/√3 but only for square matrices**, not for rectangular matrices.

<br><br>
<br>
<br>
<br>
Return to [Index]
<br>
<br>
<br>
<br>


