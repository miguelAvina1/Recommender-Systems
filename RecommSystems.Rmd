---
title: "Recommender Systems Homework" 
output: html_notebook
---


## Intelligent systems <img src="Images/tecnologico-de-monterrey-blue.png" alt="Tec Logo" width="200" style="float:right"/>
#### Date: June 3rd, 2020 
<hr>
### Professor:
#### PhD. Luis Eduardo Falcón 
### Authors:
#### Miguel Aviña &nbsp;- &nbsp; A01227898
#### Arturo Perez &nbsp;- &nbsp; A01228405

<hr>

## Index

#### 0. [Importing files]
#### 1. [Global data frame building] 
#### &nbsp;&nbsp;&nbsp;&nbsp;     1.1 [Merging *geoplaces* and *rating* datasets.]
#### &nbsp;&nbsp;&nbsp;&nbsp;     1.2 [Merging *cuisine* with *geoplaces* and *rating*]
#### 2. [Descriptive Analysis]
#### 2.1 ['City' missing data]
#### 2.2 ['RCuisine' missing data]
#### 3. [Dataset cities]
#### 4. [Popular restaurants]
#### 4.1 [Total number of restaurants]
#### 5. [Popular cuisines]
#### 5.1 [Total number of cuisines]
#### 6. [Utility matrix]
#### 6.1 [Latent variable matrix (Restaurants)]
#### 7. [Recommendation with Pearson correlation]

<hr>

## Importing files

<span style="color:#0834a4; font-family:Georgia; font-size:1.3em;">
 Use the dataset from the UCI Irvine Machine Learning Repository available for download [here](https://archive.ics.uci.edu/ml/datasets/Restaurant+%26+consumer+data) and utilize the following files and variables:
</span>
<span style="color:#0834a4; font-family:Georgia; font-size:1.2em;">
<br>
&nbsp; - <b>rating_final.csv</b>: Use only variables <b>userID, placeID and rating</b>
<br>
&nbsp; - <b>geoplaces2.csv</b>: Use only variables <b>placeID, name, city, latitude and longitude</b>
<br>
&nbsp; - <b>chefmozcuisine.csv</b>: Use only variables <b>placeID and RCuisine</b>
</span>
<br><br>

We will start by importing the three previously mentioned files:
```{r}
geoPlaces <- read.csv(file = "RCdata/geoplaces2.csv", header = TRUE, sep = ',')
rating    <- read.csv(file = "RCdata/rating_final.csv", header = TRUE, sep = ',')
cuisine   <- read.csv(file = "RCdata/chefmozcuisine.csv", header = TRUE, sep = ',')
```

1. **chefmozcuisine.csv:** 
    a. Instances: 916
    b. Attributes: 2
<br><br>
1. **geoplaces2.csv:**
    a. Instances: 130
    b. Attributes: 21
<br><br>
1. **rating_final.csv:**
    a. Instances: 1161
    b. Attributes: 5

Please refer to the complete [description](RCdata/README) for more information regarding each particular attribute.
<hr>
Then, we will fileter the variables we need.
```{r}
library(dplyr)
geoPlaces <- select(.data = geoPlaces, placeID, name, city, latitude, longitude)
rating    <- select(.data = rating, userID, placeID, rating)
cuisine   <- select(.data = cuisine, placeID, Rcuisine)

```
From now on, we will be using the names *geoPlaces*, *rating* and *cuisine* to refer to the dataframes we have created.

1. **cuisine:** 
    a. Instances: 916
    b. Attributes: 2
<br><br>
1. **geoPlaces:**
    a. Instances: 130
    b. Attributes: 21
<br><br>
1. **rating:**
    a. Instances: 1161
    b. Attributes: 5
    
<hr>
By displaying the first entries of each dataframe we obtained a general idea of the data.
```{r}
head(geoPlaces)
```
<center>**Figure 1. First entries of geoPlaces**</center>
```{r}
head(rating)
```
<center>**Figure 2. First entries of rating**</center>
```{r}
head(cuisine)
```
<center>**Figure 3. First entries of cuisine**</center>
<br>
<br>
We noticed the following aspects:
<br>

1. Some entries of the 'city' attribute refers to the same city but with different notation. <br>
    *E.g:* "s.l.p" and "San Luis Potosi". We might consider removing this attribute or
    unifying these different notations into a single name for each city.
    
1. 'name' attribute is listed in a natural human-readble way.
    Further analysis using this kind of format could be problematic. We might consider
    removing this attribute and kept only with placeID.
<hr>

## Global data frame building
<span style="color:#0834a4; font-family:Georgia; font-size:1.3em;">
From the imported files, generate a single data frame with the previously indicated variables
</span>

Before performing a merge between the three dataframe we have so far, we observed that there are 
different number of observations in each one of the them:
```{r}
sprintf("Number of Observations in cuisine: %s",dim(cuisine)[1])
sprintf("Number of Observations in geoPlaces: %s",dim(geoPlaces)[1])
sprintf("Number of Observations in rating: %s",dim(rating)[1])
```
<br>
In *rating* we expect to have repeated observations (or entries) for both UserID and placeID, because each user could have rated more than one place, and, similarly, more than one place could have had many ratings (score) from different users.
<br>
<br>
For clarity, we will extract the number of **unique** userID and placeID, which will tell us **how many different users and restaurants are in the dataset**.
```{r}
rating_UserIDs <- unique(rating$userID)
sprintf("Number of unique userID: %s", nlevels(rating_UserIDs))
```
<br>
From this point, we know that we are handling score data from **138 different users**. The UserID increments from U1001 to U1138.
```{r}
rating$placeID <- as.factor(rating$placeID)
sprintf("Number of unique placeID: %s",nlevels(rating$placeID))
```
<br>
We have also found that there are **130 different restaurants**. The ID does not increase in an ordered way, so there are gaps in the PlaceID. This indeed correspond to the number of observations of *geoPlaces* we displayed before.
<br>
<br>
RStudio provides an easy way of visualizing the data we are handling. For each data frame, it displays each one of their variables including its type and the first entries. By using this graphical interface (Fig. 4), we noticed that there are 129 levels of the variable "name" in the *geoPlaces*, but we expect 130, one for each placeID. This might mean that there is a repeated name. We will analyze the data to confirm this hypothesis.<br><br>

<div style="text-align: center">
<img src="Images/geoPlacesDataFrame.PNG" alt="R Studio GUI" width="700" />
</div>
<center>**Figure 4. RStudio data frame visualization**</center>

```{r}
n_occur <- data.frame(table((geoPlaces$name)))
n_occur[n_occur$Freq > 1,]
```
<center>**Figure 5. Repeated restaurant**</center>
<br>
<br>
We could see that Gorditas Dona Tota is repeated, this might not be interesting but it is good to know to avoid any misinterpretation in the future.
```{r}
geoPlaces[geoPlaces$name %in% n_occur$Var1[n_occur$Freq > 1],]
```
<center>**Figure 6. Information about the repeated restaurant**</center>
<br>
<br>
Finally, we will analize the data in the *cuisine* dataframe. What we would expect here is to have the same 130 levels for placeID as in *rating* and *geoPlaces* dataframes. As we see in Fig. 7, the variable "Rcuisine" is a 59 level factor, which tell us that there are 59 different cuisines in the dataset.
We can also observe that we have 916 observations in *cuisine* dataframe while we are only dealing with 130 restaurants. 
<br>

<div style="text-align: center">
<img src="Images/cuisineDataFrame.PNG" alt="R Studio GUI" width="700" />
</div>

<center>**Figure 7. RStudio cuisine data frame visualization**</center>

```{r}
cuisine$placeID <- as.factor(cuisine$placeID)
sprintf("Number of different placeID: %s",nlevels(cuisine$placeID))

```
<br>
By converting placeID to factor, we noticed that there are 769 different placeID's. This means that the *cuisine* dataframe has information of restaurants that we don't have in the other two dataframes. By doing the intersection between rating\$placeID and cuisine\$placeID, we can know which restaurants (placeIDs) they have in common.
```{r}
commonPlaceIDs <- as.factor(intersect(cuisine$placeID, rating$placeID))
sprintf("Intersection between rating and cuisine: %s",nlevels(commonPlaceIDs))
```
<br>
We were expecting to have an intersection of 130; it would have mean that *cuisine* dataframe has the cuisine information for all the places we have in the other two dataframes. However, we found that they only intersect in 95 places, meaning that **we don't have cuisine information of 35 restaurants**.
Further, since *cuisine* have 916 observations but only 769 placeID's, we could conclude that **there are more than one cuisine assigned to each placeID**.

We will filter the repeated placeID's with more than one occurence in *cuisine* in order to get a better understanding of this data frame. Below we display the most repeated placesID's.
```{r}
n_occur <- data.frame(table(cuisine$placeID))
n_occur <- n_occur[order(-n_occur$Freq),]
head(n_occur[n_occur$Freq > 1,])
```
<center>**Figure 8. Frequencies of most repeated restaurants**</center>
<br>
<br>
The table before showed us that there are restaurants that appears more than two times, even nine times in the *cuisine* dataframe. We will have a look of some of the repeated entries in the dataset:
```{r}
cuisine[cuisine$placeID %in% n_occur$Var1[n_occur$Freq > 1],][1:10,]
```
<center>**Figure 9. Repeated entries in cuisine dataframe**</center>
<br>
From the previous table we now have an understanding on how the data is listed. We observe that there's a row per each cuisine a restaurant has.
<br>
Let's have a look of the most repeated restaurant in *cuisine*:
```{r}
filter(cuisine, placeID == n_occur$Var1[1])  
```
<center>**Figure 10. Most repeated placeID in cuisine**</center>
<br>
After this analysis of the three imported data sets, we have discovered the following:
<br>

1. *rating* dataframe and *geoPlaces* dataframe have the same set (130) of restaurants (PlaceIDs), as we would expect.

1. *cuisine* dataframe has a larger set of restaurants (769) than *rating* and *geoPlaces*

1. Not all restaurants listed in *rating* and *geoPlaces* dataframes are in *cuisine*: **we have restaurants from which we don't know their cuisine**

1. In *cuisine*, some placeIDs have more than one cuisine linked to them.
<br>

### Merging *geoplaces* and *rating* datasets.

In order to create the global dataframe needed (recall that, to have a dataframe, we need the same number of observation in each variable) we will start by merging *geoPlaces* and *rating* using the variable "placeID' as pivot. This would add the "name", "city", "latitude" and "longitude" information for each placeID. We will perform this using the merge() function without any preprocessing of the data in the two dataframes.
Notice that, we have 130 observations in *geoPlaces* and 1161 observations in *rating* so we will expect a merged dataframe with 1161 observations.
<br>
```{r}
places <- merge(rating, geoPlaces, by="placeID")
sprintf("Number of observations in merged dataframe: %s",dim(places)[1])
```
<br>

### Merging *cuisine* with *geoplaces* and *rating*
We can not merge the *cuisine* dataframe directly as we have done in the previous section. Doing it this way would result in:

1. Dropping all the non-useful placeIDs from *cuisine*. The ones that are not part of the 130 placeIDs of *rating* and *geoplaces*, and now in the new dataframe *places*.

1. **Extra 'votes' (ratings) will be added to the merged result because *cuisine* have more than one entry per placeID.**

<br>
Doing this will alter the information and will result in wrong results when getting the [Popular Restaurants] and, in general, to all the analysis beyond this point.
<br>
<br>
However, we will demonstrate what would happen if we merge *cuisine* directly without any modification.
<br>
We will get one placeID to focus on. We will choose a PlaceID that exists in both *rating* and *cuisine* dataset and that is repeated (have more than one cuisine) in *cuisine*.
<br><br>
Getting all placeIDs' with more than one cuisine assigned to them.
```{r}
z <-  cuisine[cuisine$placeID %in% n_occur$Var1[n_occur$Freq > 1],]
```
<br><br>
Extracting the first observation for reference
```{r}
examplePlaceID <- intersect(commonPlaceIDs, z$placeID)[1]
```
<br><br>
As shown in Fig. 11, this particular placeID has 10 votes (ratings) made by 10 different users.
```{r}
filter(rating, placeID == examplePlaceID)
```
<center>**Figure 11. *rating* subset: PlaceID 135086 votes.**</center>
<br>
<br>
Furthermore, as shown in Fig. 12, the user U1108 has voted for this placeID 135086 once, giving the place a rating of '1'.
```{r}
examplePair <- filter(rating, placeID == examplePlaceID)[1,1:3]
exampleUserID <- (examplePair[1])
examplePair
```
<center>**Figure 12. Example of specific userID-placeID**</center>
<br>
<br>
Now, we will merge *cuisine* with *places* (Recall that *places* is the merged dataframe of *rating* and *geoPlaces*) which will result in an inadequate merged dataframe.
```{r}
badMerge <- merge(places, cuisine, by="placeID")
```
<br>
In Fig. 13 we can see the same placeID (135086) in the merged result. Observe how it went from having 10 votes (Fig. 11) to 20 votes.
```{r}
z <- filter(badMerge, placeID == examplePlaceID)
z
```
<center>**Figure 13. Examples of reapeated votes**</center>
<br>
<br>
Inf. Fig. 14 we can see the same UserID-PlaceID record shown in Fig. 12. Observe how this userID has two votes for the same placeID. We conclude that this merge can not be done this way since it will affect the original data and lead to bad analysis results. Particulary, what will happen is that placeID's with *n* cuisine assigned to them will increase its votes and popularity *n* times.
```{r}
filter(z, userID ==  exampleUserID[1,1])
```
<center>**Figure 14. Same userID-placeID as Fig. 12 in the merged dataframe**</center>
<br>
<br>
<br>
Preserving cuisine information without modifying the ratings can be done in many different ways. We will focus on doing what we will need for this homework, however, it might not be the best solution if further analysis need to be done with cuisine information. Our solution will be to merge placeIDs' cuisine information into a single level. Taking the example n Fig. 14, placeID will be combined into:
<br>

<div style="text-align: center">
<img src="Images/ExpectedMerge.PNG" alt="ExpectedMerge" width="1000" />
</div>
<center>**Figure 15. Desired merge for a placeID with Fast_Food and Burgers cuisine information**</center>
<br>
<br>
We are taking this aproach because there are other levels already with this kind of syntax. *E.g.:* Bar_Pub_Brewery.
<br>
<br>
We start by keeping just the placeIDs from *cuisine* we are interested in (the 130 listed in *geoPlaces*):
```{r}
cuisine <- filter(cuisine, placeID %in% commonPlaceIDs)
```
<br>
After this filter, we end up with 112 observations. Now, we will combine this observations into the real 95 unique placeIDs that we have info. After this process, we will have an useful cuisine data for the 130 places we are analyzing (Remember that we will be still having 35 places without cuisine information).
```{r}
merged_Places <- list()
merged_Cuisines <- list()
#cuisine_merged_Places <- c(cuisine_merged_Places, 3)

for (place in commonPlaceIDs) { # We will iterate along each placeID we have 
  currentPlaceID <- filter(cuisine, placeID == place)    # and will examine one by one
  new_level2 <- as.character(currentPlaceID$Rcuisine[1])
  currentPlaceID <- currentPlaceID[-c(1),]
  for (cuisineKind in currentPlaceID$Rcuisine) {
    new_level2 <- paste(new_level2, as.character(cuisineKind), sep='_')
  }
  merged_Places <- c(merged_Places, place)
  merged_Cuisines <- c(merged_Cuisines, new_level2)
}
cuisine <- data.frame('placeID' = unlist(merged_Places, use.names = FALSE), 'cuisine'=unlist(merged_Cuisines, use.names = FALSE) )
rm(merged_Places)
rm(merged_Cuisines)
rm(currentPlaceID)
```
<br>
Finally, we will merge this data with *places*
```{r}
places_drop <- merge(places, cuisine, by="placeID")
uniquePlaceID <- droplevels(as.factor(unique(places_drop$placeID)))
places <- merge(places, cuisine, by="placeID", all=TRUE)
levels(places[,8]) <- c(levels(places[,8]),"?")   # we are adding '?' to tell that we dont have that info
places[is.na(places)] <- '?'
```

Here we can see the first entries of the merged dataset and a summary:
```{r}
head(places)
```
<center>**Figure 16. First entries of the merged data**</center>
<br>
```{r}
summary(places)
```
<center>**Figure 17. Summary of merged dataset**</center>
<br>
<hr>


### Descriptive Analysis
<span style="color:#0834a4; font-family:Georgia; font-size:1.3em;">
Realize a descriptive analysis of the variables and indicate if there are missing data in any of them. If so, realize an analysis to determine and justify the decision you take regarding those missing data.
</span>

<br>

#### **placeID**

<br>
This variable was numeric and we already changed it to factor because it must be taken as a qualitative variable.
```{r}
sprintf("Number of places in the dataset: %s",nlevels(places$placeID))
```
<br>
In previous homeworks, we analyzed the proportion of observations per level, however, this does not make sense in this homework  since it would not be useful to combine them, furthermore, the analysis we will do later does not benefit from merging levels. Some level's proportions are shown in Fig. 18; however, this data is not very useful.
```{r}
head(prop.table(table(places$placeID)))
```
<center>**Figure 18. Proportions of some placeIDs**</center>
<br>
In Figure 19, we can observe a barplot of the places and their absolute frequencies. 

This kind of graphic is useful, it shows a nice
representation of the number of occurences for each placeID, that is, the popularity of the restaurants. 
<br>

```{r}
barplot(height = table(places$placeID))
``` 
<center>**Figure 19. Barplot of number of occurences for each placeID**</center>
<br>
<br>
Graphically, we can also observe that it seem to be a mean value of ~ 5 ratings per place, lets get this number right.
```{r}
library('plyr')
t <- count(places, 'placeID')
sprintf("Average votes per placeID: %s", mean(t$freq))
```
We know that 'placeID' is a categorical variable, however, the number of occurences per each placeID are basically the number of votes per restaurant and this data is numerical so we will analyze it:
```{r}
summary(t$freq)
```
The interpretation we can get from the previous summary is that there's a restaurant with 36 votes and that the restaurant with fewer votes has just three. Using the numerical data from the number of votes per restaurants, we created the boxplot shown in Fig. 20. In the boxplot we can graphically identify some atipical values, these atipical values could be seen and interpreted as outstanding (or very popular) restaurants. We are not going to do anything with this outliers because removing them does not make any sense sice we would just lose important data. 
<br>
```{r}
boxplot(t$freq)
```
<center>**Figure 20. Boxplot of placeID**</center>
<br>

#### **userID**

<br>
'UserID' was already a categorical (factor) variable, so we didn't do any modification.
<br>
We'll do the same analysis for this variable as we did for placeID because they are fair similar variables.
```{r}
barplot(height = table(places$userID))
```
<center>**Figure 21. Bar plot of number of places an user has rated**</center>
<br>
<br>
In Fig. 21 we can observe the number of times each userID appears in the data, this can be interpreted as the number of places each user has rated. It doesn't show too many uncommon users; altough there are some larger bars, they doesn't stand out as much as the larger bars from placeID in Fig. 19. 
<br>
As we did for placeID we will get the average places rated per user and the summary of the data plotted in Fig. 21.

```{r}
t <- count(places, 'userID')
sprintf("Average places rated per User: %s", mean(t$freq))
```

```{r}
summary(t$freq)
```

```{r}
boxplot(t$freq)
```
<center>**Figure 22. Boxplot of occurences of each userID**</center>
<br>
<br>
In Fig. 22 we observe what we mentioned before: there are not uncommon users. Nobody rated too much or too little to become an outlier.
<br>
<br>

#### **Rating**
<br>
Rating was imported as a numerical variable. If we check the dataframe using RStudio, we can manually observe that this variable has only three values: 0, 1, 2. Turning it into categorical (factor) variable would be better.
<br><br>
However, lets assume we have a significantly larger dataset and manually inspecting the data from the dataframe wouldn't be feasible. Then, we would start by plotting the rating as histogram (Fig. 23). After looking at this representation we would be sure that there are only three levels, so a conversion to factor must be done. Even if we would have decided to use a density plot (Fig. 24) instead of a bar plot, we could have also known that there are only three levels in the data. These examples are provided to demonstrate that it is not necessary to look directly into the observations of the dataframe to know if a variable must be converted to factor.
```{r}
hist(places$rating)
```
<center>**Figure 23. Histogram of rating**</center>
<br>
<br>
```{r}
plot(density(places$rating))
```
<center>**Figure 24. Density plot of rating**</center>
<br>
<br>
<br>
Now we are going to convert this data to a factor.
```{r}
places$rating <- as.factor(places$rating)
```
In the summary shown below, we we right in assuming that there were only three factors. Also, the the number of occurences of each level is shown.
```{r}
summary(factor(places$rating))
```
<br>
Now it makes sense to check the proportion of observations per level. In the table below we can observe that there is a good distribution of observations per level (>10%). This 10% threshold is useful when creating models (as we did for titanic and germancredit). Even if we didn't have this distribution, we would not merge the rating's levels.
```{r}
prop.table(table(places$rating))
```

<br>

#### **Name**

<br>
There are not many options we can do to analyze this variable. Below we are just looking at some of the first observations of this variable.
```{r}
head(places$name)
```
Each place name is correlated to a placeID, therefore, this variable helps to have a better identifier to the numerical placeID. 
<br>
<br>

#### **City**
```{r}
sprintf("Number of cities: %s",nlevels(places$city))
```
```{r}
levels(places$city)
```
<center>**Figure 25. Levels of 'city' variable**</center>
<br>
<br>
After displaying the city's level, it is evident that same places are taken different because of typos or extra spaces. It is important to remark that looking to the all levels is only useful when handling few of them, if we had larger sets with a lot of levels, it wouldn't be possible. 
<br>
Also, in this case we could simply use the latitude and longitude data to retrieve the city and just ignore the original imported city variable.
However, we are going to assume that we do not have latitude nor longitude, therefore, we will merge the levels manually:
<br>
If we had significantly more levels, we may try to get the correlation between level names to identify same cities with typos or written a bit different, however, this is out of the scope of this homework due to time limitations.
```{r}
places$city <- factor(places$city, labels=c("Undefined","Cd. Victoria","Cd. Victoria","Cd. Victoria",
                                            "Cuernavaca","Cuernavaca","Jiutepec", "San Luis Potosi",
                                            "San Luis Potosi", "San Luis Potosi", "San Luis Potosi",
                                            "San Luis Potosi", "San Luis Potosi", "San Luis Potosi",
                                            "Soledad", "Cd. Victoria", "Cd. Victoria"))
```
After manually merging the levels, we end with six levels which are:
```{r}
levels(places$city)
```
<center>**Figure 26. Levels of 'city' variable after merging levels**</center>
```{r}
prop.table(table(places$city))
```
<center>**Figure 27. Proportion of each city level"**</center>
<br>
<br>
From Fig. 27 we observe that most of the places are located in San Luis Potosi and none of the other levels have more than 10% of the total observations.
<br>
Now, we will do the same analysis as we did for placeID and userID.
```{r}
barplot(height = table(places$city)) 
```
<center>**Figure 28. Barplot of 'city' variable**</center>
<br>
<br>
```{r}
t <- count(places, 'city')
summary(t$freq)
```
Getting the summary  is useful to know how many ratings (votes) have the most 'popular' city (834) and that the city with fewest votes only has 17 among all its restaurants.
```{r}
boxplot(t$freq)
```
<center>**Figure 29. Boxplot of cities** </center>
<br>
<br>
The boxplot shown inf Fig. 29 only shows the outlier value which correspond to San Luis Potosi, the city with most restaurants in the data.
<br>

#### **Latitude**

<br>
This is an strictly numerical value, so we can do a numerical analysis
```{r}
print(summary(places$latitude))
```
```{r}
sprintf("Std dev: %s", round(sd(places$latitude, na.rm = TRUE), digits = 4))
```
```{r}
boxplot(places$latitude, ylab = "Lat", xlab = 'Latitude')
```
<center>**Figure 30. Boxplot of Latitude**</center>
```{r}
hist(places$latitude)
```
<center>**Figure 31. Histogram of latitude**</center>
<br>
<br>
In the histogram shown in Fig. 28 we observe that the latitude is divided in three different degrees. This shows that among the five cities we are analyzing, at least two of them share a similar latitude.
```{r}
plot(density(places$latitude, na.rm = TRUE))
```
**Figure 32. Density plot of latitude"**
<br>
<br>
In Fig. 32 we can observe four 'peaks', so there may be locations in 4 different latitudes and two of them are fair close to each other.
<br>
<br>

#### **Longitude**


```{r}
print(summary(places$longitude))
```
```{r}
sprintf("Std dev: %s", round(sd(places$longitude, na.rm = TRUE), digits = 4))
```
```{r}
boxplot(places$longitude, ylab = "Long", xlab = 'Longitude')
```
**"Figure 30. Boxplot of longitude"**
<br>
<br>
```{r}
hist(places$longitude)
```
**"Figure 31. Histogram of longtide"**
<br>
<br>
In this plot we observe as we have divided in four different main locations regarding longitude
```{r}
plot(density(places$longitude, na.rm = TRUE))
```
**"Figure 32. Density plot of longitude"**
<br>
<br>
Here we have 4 'peaks', so there may be locations in 4 different longitude
<br>
<br>
Latitude and Longitude did not give a lot of information, but normally, these data is used at the same time, we plot them.
```{r}
plot(places$longitude, places$latitude)
```
**"Figure 33.Graph of latitude and longitude"**
<br>
<br>
Now we can easily observe three main cumulus, but we have 5 different cities (and 1 undefined)
<br>
Now, let's get context. Cuernavaca is a city which adjoints Juitepec, that's why they will be seen as a main cumulous. 
<br>
And Soledad, (soledad de Graciano Sanchez) is also adjacent to San Luis Potosi, that's why those two merges in one single main cumulus
<br>
We think this kind of analysis is more suitable for this information, rather than mean, quartile, etc.
<br>
Furthermore, if we 'zoom in' we can actually observe the two comulus of juitepec and cuernavaca
<br>
<br>
**Cuisine**
<br>
<br>
This was already a categorical (factor) variable, so we didn't do any modification
```{r}
barplot(height = table(places$cuisine))
```
**"Figure 34.Graphic of cuisine"**
<br>
<br>
There is not so much information in this graph.
```{r}
t <- count(places, 'cuisine')
sprintf("Average places per cuisine: %s", mean(t$freq))
summary(t$freq)
```
So we have just 4 places of a not so popular cuisine and the maximum is regarding the '?'
```{r}
outvals <- boxplot(t$freq)$out
```
**"Figure 35. Boxplot of cuisine"**
```{r}
t[which(t$freq %in% outvals),]
```

**"Figure 36. Cuisine of all Restaurants"**
<br>
<br>
<hr>

### 'City' missing data

Perform an analysis to determine and justify the decision you make about such missing data
<br>
we have missing data in city (which came with the dataset since we imported it) and missing data in cuisine, that we added when merging cuisine data.
<br>
We could end without missing data in this variable if we haven't added the merge(places, cuisine, by="placeID", all=TRUE) 'all=TRUE' parameter to the merge operation, otherwise we would've ended with no missing data but fewer observations. 
<br>
So, for city we will use revgeo and ggmap to get the city name from lattitude and longitude
```{r}
#install.packages("revgeo")
#install.packages("ggmap")
library(revgeo)
library(ggmap)
```
First with revgeo
```{r}
undef_cities <- filter(places, city == 'Undefined') 
undef_cities <- select(.data = undef_cities, placeID, city, latitude, longitude)
```
Here we store a copy of those places without City, we will use it later.
<br>
R doesn't allow to add new entries in a factor if they don't belong to the currently levels. So first, we wil un-factorized city, then add the new cities we get, and then factorize again.
```{r}
places$city <- as.character(places$city)
for (i in 1:length(places$city)) {
  if (places$city[i] == 'Undefined') {
    possibleCity <- revgeo(places$longitude[i], places$latitude[i])
    places$city[i] =  unlist(strsplit(as.character(possibleCity), ", "))[2]
  }
}

```
**"Figure 37. Getting data from revgeo"**

Re-factorize
```{r}
places$city <- as.factor(places$city)
sprintf("Number of places: %s",nlevels(places$city))  # 
levels(places$city)
```
**"Figure 38. Same cities with different labels"**

We see that as before, we have same cities with different labels, we we will correct them as before.
```{r}
places$city <- factor(places$city, labels=c("Cd. Victoria","Cd. Victoria","Cuernavaca","Jiutepec",
                                            "San Luis Potosi", "San Luis Potosi", "Soledad", "Soledad"))
```
```{r}
places$city <- droplevels(places$city)
sprintf("Number of places: %s",nlevels(places$city))  # 
levels(places$city)
```
**"Figure 39. Cities"**
<br>
<br>
Now we will use google to make sure we are ok, we should be getting the same cities.
<br>
To use google ggmap, it requires an API key so we must register so:
<br>
First, we had to go to https://cloud.google.com/maps-platform/ to register for a free trial
<br>
We start with 300 USD so we must use it wisely
<br>
Then we must enable the google APIs for maps which will generate an API key which we used to get the required information. 
<br>
Once we have the private key, we tell ggmap about this key using:
```{r}
register_google(key = "AIzaSyD4mKmiXYdtE9xALBocEVu_Tl15a4iEW4I")
```

This key must be kept private and not shared because google has billing credit card so anyone could do bad things to your account. We recommend to secure your private key to be used only from a certain IP.

```{r}
places$GG_result <-  'Not compared'
for (i in 1:length(places$city)) {
  #
  # SO, FOR GG MAP, GGMAP returns a list with all posible addresses it found. However the structure is NOT the same for each
  # address it returns. So, we can not search of a given element inside the lists of lists it returns. We would need to use another 
  # approach. One non-efficient but useful approach would be to loop inside all the info we get when calling revgeocode to search
  # for any coincidence to the data we retrieved from RevGeo, if we find it, then we can be sure we get the correct city
  # using revgeo. Otherwise, we would need to do a manual inspection of the data received from revgeocode
  # The search would be basically a deep-first search
  if (places$placeID[i] %in% undef_cities$placeID) {   # We are just going to comprobate with those cities that were undefined before
    google_result <- revgeocode(c(places$longitude[i], places$latitude[i]), output = "all")$results
    found <- FALSE
    citynameOriginal <- places$city[i]
    compare_city <- as.character(lapply(as.character(citynameOriginal), tolower))
    compare_city <- gsub("[^a-z]","",compare_city)
    print(compare_city)
    #sprintf("Comparing with '%s'", compare_city)
    for (j in 1:length(google_result)) {
      address_comp <- google_result[[j]]$address_components
      for (k in 1:length(address_comp)) {
        long_name <- as.character(lapply(as.character(address_comp[[k]]$long_name), tolower))
        long_name <- gsub("[^a-z]","",long_name) 
        short_name <- as.character(lapply(as.character(address_comp[[k]]$short_name), tolower))
        short_name <- gsub("[^a-z]","",short_name)
        # We are not going to compae if the compare_city is at least substring of the google's result
        # We are not comparing it letter to letter because we found that google returns 'Soledad de Graciano Sanchez'
        # instead of just 'Soledad', which is the name of the city we got from revgeo. Therefore, comparing it using the
        # '==' operator will return FALSE, and it should be true, So we are going to use grep to check if one is substring
        # of the other
        if (grepl(compare_city, long_name, fixed = TRUE) || grepl(compare_city, short_name, fixed = TRUE)) {
          # print("FOUND:")
          # print(long_name)
          found = TRUE
          break
        }
      }
      if (found == TRUE) {
        break
      }
    }
    if (found == TRUE) {
      places$GG_result[i] = as.character(citynameOriginal)  # Just put the same city 
    } else {
      places$GG_result[i] <- 'Not the same'  # FUrther analysis will be needed
    }
  }
}
```
So, let's check if the results fom Google and revgeo differs, if so, we are going to do something to choose among the different cities. 
<br>
For this, if we do not find the city given by revgeo inside the payload that google gives, we assign to the dataframe the string "Not the same".
```{r}
differentResults <- filter(.data = places, GG_result == 'Not the same')
```
Lets analyze these cases where we didn't get the same from google and revgeo
```{r}
differentResults <- select(differentResults, -userID, -rating)
differentResults <- unique(differentResults)
differentResults
```

So we didn't get any result diffeent from google from what we have already got from revgeo
<br>
Now we have two columns, in city we have merged the info we had from the beginnign about city and now we have filled the missing data using the first package revgeo.
<br>
The other column is called GG_result, and there we have either: 'Not compared" because the city info wasn't missing since we imported the data set. "Not the same" found inconsistencies from what we get from regveo and google. Otherwise, it insert the same name as in city, simbolyzing that we got the same city data both from regveo and google.

### 'RCuisine' missing data

We don't  do anything regarding the cuisine because there's no way of knowing but researching on internet.
This is out of our scope becase 1. There's not needed for further analysis 2. Time contraints (Redactalo bonito LA)
3. They won't case any NA-related problem since we replace missing data with character '?'

<br>
<hr>
**3. What cities are the study restaurants from?**
<br>
To know this, we just need to refer to the variable cities
```{r}
sprintf("There are: %s cities",nlevels(places$city))
levels(places$city) 
```
<br>
<hr>
**4.If we consider the popularity of a place as those with the highest number of user reviews (regardless of whether the review was positive or negative), get the names of the 10 most rated / popular restaurants. How many different restaurants are there in total?**
<br>
Here, we just need to get the number of occurencies (observations) per placeID, since there is one observation per vote.
```{r}
t <- count(places, 'placeID')  # Getting frequency of each placeID
t <- merge(t, places, by='placeID')

t <- select(t, placeID, freq, name)
t <- unique(x = t)


t <- t[order(-t$freq),]  # Ordering in descending order
popularPlaceID <- t$placeID[1:10]  # Here we have the 10 most popular places ID

mostPopular <- subset(places, placeID %in% popularPlaceID) # Here we extract the entire row where the 
# most popular placeID appears. Now we just want 1 row per name

mostPopular <- droplevels(unique(mostPopular$name))
mostPopular <- data.frame(popularPlaceID, mostPopular, t$freq[1:10])
colnames(mostPopular) <- c("placeID", "Restaurant", "Votes")
mostPopular
```
**"Figure 40. Top 10 rated restaurants"**
<br>
<br>
**4.1 As an extra part, let's get those 'best rated' restaurants, this is the one with most '2' as rating.**
```{r}
best_rated <- filter(places, rating == "2")
best_rated <- select(.data = best_rated, placeID, name)
t <- count(best_rated, 'placeID')  # Getting frequency of each best-rated placeID
t <- t[order(-t$freq),] 

bestRatedPlaceID <- t$placeID[1:10]  # Here we have the 10 most popular and best rated places ID
bestRated <- subset(best_rated, placeID %in% bestRatedPlaceID) # Here we extract the entire row of top 10 best rated places
bestRated <- droplevels(unique(bestRated$name))
bestRated <- data.frame(popularPlaceID, bestRated, t$freq[1:10])
colnames(bestRated) <- c("placeID", "Restaurant", "Positive Votes")
bestRated
# Compare with
```
**"Figure 41. Best Rated Restaurants"**
```{r}
mostPopular
```
**"Figure 42. Compared with the most popular"**
<br>
<br>
<hr>
**5. What type of food / cuisine are the 10 most popular restaurants found in the previous paragraph? How many different types of cuisine are there in total?**
<br>
<br>
Here we can obtain the type of combined kitchen that we created at the beginning:
```{r}
mostPopular <- merge(mostPopular, cuisine, by='placeID')
mostPopular
```
**"Figure 43. Most popular restaurants"**
<br>
<br>
Getting only the cuisine
```{r}
levels(droplevels(mostPopular$cuisine))
```
**"Figure 44. Most popular cuisines"**
<br>
<br>
**How many different types of cuisine are there in total?**
<br>
<br>
Here we can have three different results
<br>
<br>
**a. Types of food food inside the dataset of the 130 places that are already merged.**
```{r}
sprintf("Number of types of cuisines: %s",nlevels(droplevels(places$cuisine)))
levels(places$cuisine)
```
**"Different types of cuisines"**
<br>
<br>
**b. Types of cuisine inside the dataset that was imported, without processing.**
<br>
Since we have made a lot of modofications, we will import the dataset again.
```{r}
cuisine_original   <- read.csv(file = "RCdata/chefmozcuisine.csv", header = TRUE, sep = ',')
cuisine_original   <- select(.data = cuisine_original, placeID, Rcuisine)
sprintf("Number of types of cuisine: %s",nlevels(cuisine_original$Rcuisine))
```

Recall that we have this many cuisine types because in this dataset we have placesID we are not analyzing.
<br>
**c. Types of cuisine inside the dataset of original cuisine, just the 130 places that we analyzed.**
```{r}
commonPlaceIDs <- as.factor(intersect(cuisine$placeID, rating$placeID))
cuisine_original_130 <- droplevels(subset(cuisine_original, placeID %in% commonPlaceIDs))
sprintf("Number of types of cuisine: %s", nlevels(cuisine_original_130$Rcuisine))
levels(cuisine_original_130$Rcuisine)
```
**"Figure 45. Types of cuisine of the original dataset"**
<br>
<br>
<hr>
**6. Generate the utility matrix considering the rows with the variable userID, the columns with placeID, and the values of the matrix with rating. From this utility matrix, apply SVD (Singular Value Decomposition) factorization to obtain the latent variable matrix for restaurants.**
<br>
<br>
First, let's get only the variables we will need in the simple-triple-matrix form
```{r}
places_stm <- select(places, placeID, userID, rating, name)
head(places_stm)
```
**"Figure 46. Variables needed for matrix"**
<br>
<br>
Altough we used rating as a categorical value previously, here we must convert to numerical again
```{r}
places_stm$rating <- as.numeric(places_stm$rating)
utMx <- with(places_stm, tapply(rating, list(userID, name), sum, default=NA))
sprintf("Dimension of the matrix: %s",dim(utMx))
```
We can observe that we created a matrix of 138x129, 138 correspond to the users we know we have but 
<br>
We should have 130 restaurant names, where is the missing one?
<br>
If we recall, at the begining we identified two places with the same name. Until know, it hasn't been problematic but know, let's just change one of those places' name so we can identify it as separate restaurants
<br>
Let's retrieve that name again
```{r}
n_occur <- data.frame(table((geoPlaces$name)))
n_occur[n_occur$Freq > 1,]
```
**"Figure 47. Repeated restaurant"**
<br>
<br>
```{r}
geoPlaces[geoPlaces$name %in% n_occur$Var1[n_occur$Freq > 1],]
```
<br>
<br>
```{r}
# Let's get the placeID of the first one
ID <- geoPlaces[geoPlaces$name %in% n_occur$Var1[n_occur$Freq > 1],][1,1]
ID
```
**"Figure 48. placeID of one of the repeated restaurants"**
<br>
<br>
Now, we will replace "Gorditas Dona Tota" to "Gorditas Dona Tota 2"
<br>
De-factorize
```{r}
backup <- places$name
places$name <- as.character(places$name)
for (i in 1:length(places$placeID)) {
  if (places$placeID[i] == ID) {
     places$name[i] = "Gorditas Dona Tota 2"
  }
}
places$name <- as.factor(places$name)
```
Repeating the process again
```{r}
places_stm <- select(places, placeID, userID, rating, name)
places_stm$rating <- as.numeric(places_stm$rating)
utMx <- with(places_stm, tapply(rating, list(userID, name), sum, default=NA))
dim(utMx)
```
```{r}
utMx[1:10, 3:6]  # Here we can observe that it is a disperse matrix
```
**"Figure 49. Disperse Matrix"**
<br>
<br>
```{r}
place_names <- colnames(utMx)
head(place_names)
```
We save the utility matrix
```{r}
write.csv(utMx, file = "RCdata/Utility_Matrix.csv", na="")
```

**From this utility matrix, apply SVD (Singular Value Decomposition) factorization to obtain the latent variable matrix for restaurants.**
```{r}
#install.packages("irlba")
library(irlba)
```

If we want to generate the matrix of latent variables for restaurants, we should end with a matrix u with dimensions 130x130
<br>
We change "NA" for 0
```{r}
utMx[is.na(utMx)] <- 0
MSVD <- svd(t(utMx))  # in MSVD we have u, d and v that forms from the decomposition
dim(MSVD$u)  # We can observe we end with the right size of u, because we transpose the utility matrix in the line before
placesLatentMatix <- MSVD$u
placesLatentMatix
placesLatentMatix[1:10,1:10] 
```

**7. If a user liked the restaurant “Gorditas Doña Gloria” with placeID: 132834, what other 12 restaurants (indicate the names) could you recommend using the SVD decomposition with the "pearson" correlation method and considering the 10 largest singular values of the SVD? Note: it doesn't matter for the moment what city be the restaurant.**

```{r}
utrunc <- MSVD$u[,1:10]
```

Truncate it for the first 10 latent columns, corresponding to the 10 largest latent values, Since the svd () function we used before returns the singular values in d ordered from highest to lowest
```{r}
corr_mx <- cor( t(utrunc), use='pairwise.complete.obs', method = "pearson")
```
Here we get the correlation matix using the pearson correlation.
```{r}
df_cor <- as.data.frame(corr_mx)
library(ggplot2)
library(reshape2)

tmp <- melt(data.matrix(cor(df_cor)))
ggplot(data=tmp, aes(x=Var1,y=Var2,fill=value)) + geom_tile()
```
**"Figure 50. Graph of correlation matrix"**
<br>
<br>
We tried to have a look to the correlation matix graphically, however, since there are a lot of variables is not recommendedd

```{r}
place_names <- colnames(utMx)
```
Now, from a restaurant name, lets get the index position in the corelation matrix:
```{r}
# We select the name
name2look <- 'Gorditas Doa Gloria'
#And obtain the index of the correlation matrix
nameIndex <- which(place_names == name2look)
```
Now we select all the column of this restaurant
```{r}
restOptions <- corr_mx[,nameIndex]
# We obtain the index organized from higher correlation to lower correlation
orderedRest <- order(-restOptions)
# And then we use these indices to extract the names associated with those indices with greater correlation
# We ignore the first index since it will be the one with correlation 1, that is, the same restaurant and we will not recommend the same restaurant
numberOfRecomm <- 12  
RecommendedRest <- place_names[orderedRest[2:numberOfRecomm+1]]

RecommendedRest # Here are the 12 recommended restaurants

```

We observe that we have truncated the decomposition by svd() to the first 10 values, this is indeed 
a way to balancing the complexity of the model mantaining its accuracy. However, we thought that cutting it to the first 10 values, that is, the first 10 vectos of the latent matrix 'u' , related to the 10 greated singula values of the vector 'd'. We indeed have lost some infromation due to this truncation, but how are we sure we truncated it to the right length to avoid missing too much information?
<br>
There are several way of calculating this truncation value, most of them are empirical, such as plotting the singular values and locating the 'elbow' of the graph:
```{r}
plot(MSVD$d)
```
**"Figure 51. Graph of singular values"**
<br>
<br>
Here we se that the elbow of the graph is not as evident, so we can not take this approach
For this homework we will keep with the 10 value truncation, bc of time factors, however, we hope to implement the Gavish and Donoho approach cited in. 


## Indication 1

